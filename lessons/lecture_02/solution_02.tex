%%% BAT solution 02 %%%
\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, enumitem, fancyhdr, tikz}
% get rid of paragraph indent
\setlength{\parindent}{0 pt}
% allow section.equation numbering
\numberwithin{equation}{section}
% allows you to copy-paste code segments. requires pygments, which can be
% installed from PyPI with pip install Pygments.
% warning: minted does NOT work with Texmaker if you have TeX Live installed
% on WSL but Texmaker installed natively on Windows!
%\usepackage{minted}
% alternative to minted that does not require Python, LaTeX only. listings is
% however disgusting out of the box and some setup is required.
\usepackage{listings, xcolor}
% makes clickable links to sections
\usepackage{hyperref}
% make the link colors blue, as well as cite colors. urls are magenta
\hypersetup{
    colorlinks, linkcolor = blue, citecolor = blue, urlcolor = magenta
}
% fancy pagestyle so we can use fancyhdr for fancy headers/footers
\pagestyle{fancy}
% add logo in right of header. note that you will have to adjust logo path!
\fancyhead[R]{\includegraphics[scale = 0.15]{../bac_logo1.png}}
% don't show anything in the left and center header
\fancyhead[L, C]{}
% give enough space for logo by reducing top margin height, head separator,
% increasing headerheight. see Figure 1 in the fancyhdr documentation. if
% \topmargin + \headheight + \headsep = 0, original text margins unchanged.
\setlength{\topmargin}{-60 pt}
\setlength{\headheight}{50 pt}
\setlength{\headsep}{10 pt}
% remove decorative line in the fancy header
\renewcommand{\headrulewidth}{0 pt}

% color definitions for listings syntax highlighting. uses colors borrowed
% from the VS Code Dark+ and Abyss standard themes.
\definecolor{KwColor}{RGB}{153, 102, 184}     % keyword color
\definecolor{VarColor}{RGB}{86, 156, 214}     % variables/identifier color
\definecolor{StrColor}{RGB}{209, 105, 105}    % string color
\definecolor{CmtColor}{RGB}{106, 153, 85}     % comment color

% general listings configuration for all languages
\lstset{
    % change keyword, identifier, comment, string colors
    keywordstyle = \color{KwColor},
    commentstyle = \color{CmtColor},
    identifierstyle = \color{VarColor},
    stringstyle = \color{StrColor},
    % no spaces in strings
    showstringspaces = false,
    % monospace font by default
    basicstyle = \ttfamily,
    % tabsize 8 by default, this is not the 1960s
    tabsize = 4,
    % add line numbers to the left with gray typewriter font
    numbers = left,
    numberstyle = \color{gray}\ttfamily,
    % change distance from code block from 10 pt to 5 pt
    numbersep = 5 pt
}

% title, author, date
\title{Solution 2}
\author{Derek Huang\thanks{NYU Stern 2021, BAC Advanced Team.}}
\date{May 20, 2021}%\thanks{Original version released on February 25, 2021.}}

\begin{document}

\maketitle
% need to include this after making title to undo the automatic
% \thispagestyle{plain} command that is issued.
\thispagestyle{fancy}

\section{Linear algebra}

\subsection{The centering matrix}

\textit{Solution.} Since $ \tilde{\mathbf{x}}_k^\top = \mathbf{x}_k^\top -
\bar{\mathbf{x}}^\top $, then $ \tilde{\mathbf{X}} = \mathbf{X} -
\mathbf{1}\bar{\mathbf{x}}^\top $. Therefore, Since $ \bar{\mathbf{x}}^\top =
\frac{1}{N}\mathbf{1}^\top\mathbf{X} $,
\begin{equation*}
    \tilde{\mathbf{X}} = \mathbf{X} -
    \mathbf{1}\left(\frac{1}{N}\mathbf{1}^\top\mathbf{X}\right) =
    \left(\mathbf{I} - \frac{1}{N}\mathbf{11}^\top\right)\mathbf{X}
\end{equation*}

\subsection{Symmetric rank-1 matrices}

\textit{Proof.} Note that $ \big(\mathbf{xx}^\top\big)^\top =
\big(\mathbf{x}^\top\big)^\top\mathbf{x}^\top = \mathbf{xx}^\top \Rightarrow
\mathbf{xx}^\top $ is symmetric. Furthermore, if we expand the columns of
$ \mathbf{xx}^\top $, we have $ \mathbf{xx}^\top = \begin{bmatrix}
\ x_1\mathbf{x} & \ldots & x_m\mathbf{x} \ \end{bmatrix} $. Since $ \mathbf{x}
\ne \mathbf{0} $, $ \exists \lambda \in \{1, \ldots m\} $ s.t.
$ x_\lambda \ne 0 $, so $ \forall k \in \{1, \ldots m\} \setminus
\{\lambda\} $, $ \forall r \in \mathbb{R}\setminus \{0\} $,
$ r(x_k\mathbf{x}) - \frac{rx_k}{x_\lambda}(x_\lambda\mathbf{x}) = \mathbf{0} 
\Rightarrow \operatorname{rank}\big(\mathbf{xx}^\top\big) < 2 $. Since the
rank of any matrix that is not the zero matrix must be $ \ge 1 $, we must have
that $ \operatorname{rank}\big(\mathbf{xx}^\top\big) = 1 $.

\subsection{The $ \ell^0 $ ``norm''}

\textit{Proof.} Let $ \mathbf{x} \in \mathbb{R}^n $ such that
$ \Vert\mathbf{x}\Vert_0 = k < n $. Then, $ \forall r \in \mathbb{R} \setminus
\{0\} $, it's clear that $ \Vert r\mathbf{x}\Vert_0 = k \ne |r|k =
|r|\Vert\mathbf{x}\Vert_0 $. That is, $ \Vert\cdot\Vert_0 $ is not absolutely
homogenous and therefore is not a proper norm.

\subsection{Vector orthogonality}

Recall that for $ \mathbf{u}, \mathbf{v} \in \mathbb{R}^n $,
$ \mathbf{u}^\top\mathbf{v} = \Vert\mathbf{u}\Vert_2\Vert\mathbf{v}\Vert_2
\cos\theta $, $ \theta \in [0, \pi] $. Prove
\begin{enumerate}[label = \alph*.]
    \item
    \textit{Proof.} $ \mathbf{u} = k\mathbf{v} $, $ k \in (0, \infty)
    \Rightarrow \mathbf{u}^\top\mathbf{v} = k\mathbf{u}^\top\mathbf{u} =
    k\Vert\mathbf{u}\Vert_2^2\cos\theta $, but since $ k\mathbf{u}^\top
    \mathbf{u} = k\Vert\mathbf{u}\Vert_2^2 > 0 $, we have
    \begin{equation*}
        k\Vert\mathbf{u}\Vert_2^2\cos\theta = \mathbf{u}^\top\mathbf{v} =
        k\mathbf{u}^\top\mathbf{u} = k\Vert\mathbf{u}\Vert_2^2 \Leftrightarrow
        \cos\theta = 1 \Leftrightarrow \theta = 0
    \end{equation*}

    Therefore, we have proven that $ \forall \mathbf{u}, \mathbf{v} \in
    \mathbb{R}^n \setminus \{\mathbf{0}\} $, $ \forall k \in (0, \infty) $,
    $ \mathbf{u} = k\mathbf{v} \Rightarrow \theta = 0 $.

    \item
    \textit{Proof.} Suppose $ \mathbf{u}^\top\mathbf{v} = 0 $. Then,
    $ 0 = \mathbf{u}^\top\mathbf{v} = \Vert\mathbf{u}\Vert_2
    \Vert\mathbf{v}\Vert_2\cos\theta \Leftrightarrow \cos\theta = 0
    \Leftrightarrow \theta = \frac{\pi}{2} $. Now suppose
    $ \theta = \frac{\pi}{2} $. Then, $ \cos\theta = 0 \Rightarrow
    \mathbf{u}^\top\mathbf{v} = 0 $. Therefore, we have proven that
    $ \forall \mathbf{u}, \mathbf{v} \in \mathbb{R}^n \setminus
    \{\mathbf{0}\} $, $ \mathbf{u}^\top\mathbf{v} = 0 \Leftrightarrow
    \theta = \frac{\pi}{2} $.
\end{enumerate}

% pushes footnote down a bit
\medskip

\subsection{QR decomposition linear least squares}

\textit{Solution.} Since the gradient of the objective is $ \mathbf{0} $ at $ \hat{\mathbf{w}} \in \mathbb{R}^d $, then as usual we have
\begin{equation*}
    -\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\hat{\mathbf{w}}) =
    \mathbf{X}^\top\mathbf{X}\hat{\mathbf{w}} - \mathbf{X}^\top\mathbf{y} =
    \mathbf{0} \Leftrightarrow
    \hat{\mathbf{w}} = \big(\mathbf{X}^\top\mathbf{X}\big)^{-1}
    \mathbf{X}^\top\mathbf{y}
\end{equation*}

Since $ \mathbf{X} = \mathbf{Q}_1\mathbf{R} $ and $ \mathbf{Q}_1^\top\mathbf{Q}_1 = \mathbf{I}_d $, then $ \mathbf{X}^\top\mathbf{X} = \mathbf{R}^\top\mathbf{Q}_1^\top\mathbf{Q}_1\mathbf{R} = \mathbf{R}^\top\mathbf{R} $, which is also invertible. Since $ \mathbf{R} $ is invertible, then $ (\mathbf{X}^\top\mathbf{X})^{-1} = (\mathbf{R}^\top\mathbf{R})^{-1} = \mathbf{R}^{-1}(\mathbf{R}^\top)^{-1} $. Noting that $ \mathbf{X}^\top = \mathbf{R}^\top\mathbf{Q}_1^\top $ and subtituting, we have
\begin{equation*}
    \hat{\mathbf{w}} = \mathbf{R}^{-1}\big(\mathbf{R}^\top\big)^{-1}\mathbf{R}^\top\mathbf{Q}_1^\top\mathbf{y} = \mathbf{R}^{-1}\mathbf{Q}_1^\top\mathbf{y}
\end{equation*}

\section{Vector calculus}

\subsection{Quadratic functions}

\begin{enumerate}[label = \alph*.]
    \item
    \textit{Solution.} Using rules from the derivative cookbook,
    $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \nabla f(\mathbf{x})
    \triangleq \mathbf{Qx} + \mathbf{a} $.

    \item
    \textit{Solution.} Using rules from the derivative cookbook,
    $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \nabla^2f(\mathbf{x})
    \triangleq \mathbf{Q} $. Since $ \mathbf{Q} \succeq \mathbf{0} $, we know
    that $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \nabla^2f(\mathbf{x})
    \triangleq \mathbf{Q} \succeq \mathbf{0} \Rightarrow f $ is a convex
    function.
\end{enumerate}

\subsection{Ridge regression}

\textit{Solution.} Since the gradient of the objective is $ \mathbf{0} $ at $ \hat{\mathbf{w}} \in \mathbb{R}^d $, we see that
\begin{equation*}
    -\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\hat{\mathbf{w}}) +
    \lambda\hat{\mathbf{w}} =
    \big(\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I}\big)\hat{\mathbf{w}}
    - \mathbf{X}^\top\mathbf{y} = \mathbf{0} \Leftrightarrow
    \hat{\mathbf{w}} = \big(
        \mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I}
    \big)^{-1}\mathbf{X}^\top\mathbf{y}
\end{equation*}

\end{document}