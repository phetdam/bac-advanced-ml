%%% BAT exercise 02 %%%
\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, enumitem, fancyhdr, tikz}
% get rid of paragraph indent
\setlength{\parindent}{0 pt}
% allow section.equation numbering
\numberwithin{equation}{section}
% allows you to copy-paste code segments. requires pygments, which can be
% installed from PyPI with pip install Pygments.
% warning: minted does NOT work with Texmaker if you have TeX Live installed
% on WSL but Texmaker installed natively on Windows!
%\usepackage{minted}
% alternative to minted that does not require Python, LaTeX only. listings is
% however disgusting out of the box and some setup is required.
\usepackage{listings, xcolor}
% makes clickable links to sections
\usepackage{hyperref}
% make the link colors blue, as well as cite colors. urls are magenta
\hypersetup{
    colorlinks, linkcolor = blue, citecolor = blue, urlcolor = magenta
}
% fancy pagestyle so we can use fancyhdr for fancy headers/footers
\pagestyle{fancy}
% add logo in right of header. note that you will have to adjust logo path!
\fancyhead[R]{\includegraphics[scale = 0.15]{../bac_logo1.png}}
% don't show anything in the left and center header
\fancyhead[L, C]{}
% give enough space for logo by reducing top margin height, head separator,
% increasing headerheight. see Figure 1 in the fancyhdr documentation. if
% \topmargin + \headheight + \headsep = 0, original text margins unchanged.
\setlength{\topmargin}{-60 pt}
\setlength{\headheight}{50 pt}
\setlength{\headsep}{10 pt}
% remove decorative line in the fancy header
\renewcommand{\headrulewidth}{0 pt}

% color definitions for listings syntax highlighting. uses colors borrowed
% from the VS Code Dark+ and Abyss standard themes.
\definecolor{KwColor}{RGB}{153, 102, 184}     % keyword color
\definecolor{VarColor}{RGB}{86, 156, 214}     % variables/identifier color
\definecolor{StrColor}{RGB}{209, 105, 105}    % string color
\definecolor{CmtColor}{RGB}{106, 153, 85}     % comment color

% general listings configuration for all languages
\lstset{
    % change keyword, identifier, comment, string colors
    keywordstyle = \color{KwColor},
    commentstyle = \color{CmtColor},
    identifierstyle = \color{VarColor},
    stringstyle = \color{StrColor},
    % no spaces in strings
    showstringspaces = false,
    % monospace font by default
    basicstyle = \ttfamily,
    % tabsize 8 by default, this is not the 1960s
    tabsize = 4,
    % add line numbers to the left with gray typewriter font
    numbers = left,
    numberstyle = \color{gray}\ttfamily,
    % change distance from code block from 10 pt to 5 pt
    numbersep = 5 pt
}

% title, author, date
\title{Exercise 2}
\author{Derek Huang\thanks{NYU Stern 2021, BAC Advanced Team.}}
\date{May 20, 2021\thanks{Original version released on February 25, 2021.}}

\begin{document}

\maketitle
% need to include this after making title to undo the automatic
% \thispagestyle{plain} command that is issued.
\thispagestyle{fancy}

\section{Linear algebra}

\subsection{The centering matrix}

Let $ \mathbf{X} \in \mathbb{R}^{N \times d} $ be a matrix where each $ k $th
row is a transposed data example $ \mathbf{x}_k^\top $. Some matrix analysis
methods require that the data be \textit{centered}, i.e. that the sample mean
$ \bar{\mathbf{x}} \triangleq \frac{1}{N}\mathbf{X}^\top\mathbf{1} =
\mathbf{0} \in \mathbb{R}^d $. Let $ \tilde{\mathbf{X}} \in
\mathbb{R}^{N \times d} $ be $ \mathbf{X} $ centered, where each $ k $th row
is $ \tilde{\mathbf{x}}_k^\top \triangleq \mathbf{x}_k^\top -
\bar{\mathbf{x}}^\top $ and $ \frac{1}{N}\tilde{\mathbf{X}}^\top\mathbf{1}
= \mathbf{0} $. Show that 
\begin{equation*}
    \tilde{\mathbf{X}} = \left(
        \mathbf{I} - \frac{1}{N}\mathbf{11}^\top
    \right)\mathbf{X}
\end{equation*}

The matrix $ \mathbf{I} - \frac{1}{N}\mathbf{11}^\top \in
\mathbb{R}^{N \times N} $ is called the \textit{centering matrix}.

\subsection{Symmetric rank-1 matrices}

The \textit{outer product} of $ \mathbf{x} \in \mathbb{R}^m $,
$ \mathbf{y} \in \mathbb{R}^n $, is $ \mathbf{xy}^\top \in
\mathbb{R}^{m \times n} $, where
\begin{equation*}
    \mathbf{xy}^\top = \begin{bmatrix}
        \ x_1y_1 & \ldots & x_1y_n \ \\
        \ \vdots & \ddots & \vdots \ \\
        \ x_my_1 & \ldots & x_my_n \
    \end{bmatrix}
\end{equation*}
Prove that $ \mathbf{xx}^\top $ is symmetric and that if $ \mathbf{x} \ne
\mathbf{0} $, $ \operatorname{rank}\big(\mathbf{xx}^\top\big) = 1 $.

\subsection{The $ \ell^0 $ ``norm''}

The \textit{counting}, or $ \ell^0 $, ``norm'' of a vector $ \mathbf{x} \in
\mathbb{R}^n $, denoted by $ \Vert\mathbf{x}\Vert_0 $, gives the number of
nonzero elements of $ \mathbf{x} $. The $ \ell^0 $ ``norm'' is called a norm
in quotations since it does not satisfy the requisite properties to be a norm,
although it does satisfy the triangle inequality. Prove that the $ \ell^0 $
``norm'' is not a norm.

\subsection{Vector orthogonality}

Recall that for $ \mathbf{u}, \mathbf{v} \in \mathbb{R}^n \setminus
\{\mathbf{0}\} $, $ \mathbf{u}^\top\mathbf{v} = \Vert\mathbf{u}\Vert_2
\Vert\mathbf{v}\Vert_2\cos\theta $, $ \theta \in [0, \pi] $. Prove
\begin{enumerate}[label = \alph*.]
    \item
    $ \forall k \in (0, \infty) $, $ \mathbf{u} = k\mathbf{v} \Rightarrow
    \theta = 0 $.

    \item
    $ \mathbf{u}^\top\mathbf{v} = 0 \Leftrightarrow \theta = \frac{\pi}{2} $.
\end{enumerate}

% pushes footnote down a bit
\medskip

\subsection{QR decomposition linear least squares}

The QR decomposition of a matrix $ \mathbf{A} \in \mathbb{R}^{m \times n} $
where $ \operatorname{rank}(\mathbf{A}) = n $ factors $ \mathbf{A} $ such that
\cite{bv_convex_opt}
\begin{equation*}
    \mathbf{A} = \begin{bmatrix} \ \mathbf{Q}_1 & \mathbf{Q}_2 \ \end{bmatrix}
    \begin{bmatrix} \ \mathbf{R} \ \\ \ \mathbf{0} \ \end{bmatrix}
     = \mathbf{Q}_1\mathbf{R}
\end{equation*}

Here $ \mathbf{Q}_1 \in \mathbb{R}^{m \times n} $, $ \mathbf{R} \in
\mathbb{R}^{n \times n} $. $ \mathbf{Q}_1 $ has orthonormal columns, i.e.
$ \mathbf{Q}_1^\top\mathbf{Q}_1 = \mathbf{I}_n $, and $ \mathbf{R} $ is
\textit{upper triangular}, i.e. all nonzero elements are on the main diagonal
and above, with nonzero diagonal elements. Given centered input data
$ \mathbf{X} \in \mathbb{R}^{N \times d} $,
$ \operatorname{rank}(\mathbf{A}) = d $, each row a transposed data example
$ \mathbf{x}_k^\top $, and centered\footnotemark\footnotetext{
    If $ \mathbf{X}, \mathbf{y} $ are already centered, i.e.
    $ \mathbf{X}^\top\mathbf{1} = \mathbf{0} $,
    $ \mathbf{y}^\top\mathbf{1} = 0 $, then we don't need to fit the bias
    term.
} response vector $ \mathbf{y} \in \mathbb{R}^N $, the linear least squares
problem can be written in matrix-vector form as
\begin{equation*}
    \begin{array}{ll}
        \displaystyle\min_\mathbf{w} &
        (\mathbf{y} - \mathbf{Xw})^\top(\mathbf{y} - \mathbf{Xw})
    \end{array}
\end{equation*}

Suppose $ \mathbf{X} = \mathbf{Q}_1\mathbf{R} $, $ \mathbf{Q}_1 \in
\mathbb{R}^{N \times d} $, $ \mathbf{R} \in \mathbb{R}^{d \times d} $. Write
an analytical expression for $ \hat{\mathbf{w}} $, the \textit{ordinary least
squares estimator}, that solves this problem. The expression must not contain
$ \mathbf{X} $.

\medskip

\textit{Hints.} The objective is convex, so its gradient is $ \mathbf{0} $ at
$ \hat{\mathbf{w}} $. $ \mathbf{R} $ and $ \mathbf{X}^\top\mathbf{X} $ are
invertible. Use properties of the matrix transpose and the fact that
$ \mathbf{Q}_1^\top\mathbf{Q}_1 = \mathbf{I}_d $. It may help to find an
expression for $ \hat{\mathbf{w}} $ first without factoring $ \mathbf{X} $
and to then substitute $ \mathbf{Q}_1\mathbf{R} $ for $ \mathbf{X} $ and
perform any additional calculations at the end.

\section{Vector calculus}

\subsection{Quadratic functions}

Given $ \mathbf{Q} \succeq \mathbf{0} \in \mathbb{R}^{n \times n} $,
$ \mathbf{a} \in \mathbb{R}^n $, $ b \in \mathbb{R} $, define function
$ f : \mathbb{R}^n \rightarrow \mathbb{R} $ such that $ \forall \mathbf{x} \in
\mathbb{R}^n $,
\begin{equation*}
    f(\mathbf{x}) \triangleq \frac{1}{2}\mathbf{x}^\top\mathbf{Qx} +
    \mathbf{a}^\top\mathbf{x} + b
\end{equation*}

\begin{enumerate}[label = \alph*.]
    \item
    Determine the form of $ \nabla f : \mathbb{R}^n \rightarrow
    \mathbb{R}^n $, the gradient of $ f $.

    \item
    Determine the form of $ \nabla^2f : \mathbb{R}^n \rightarrow
    \mathbb{R}^{n \times n} $, the Hessian of $ f $. What does it tell you
    about $ f $?
\end{enumerate}

\subsection{Ridge regression}

Given centered input data $ \mathbf{X} \in \mathbb{R}^{N \times d} $,
$ \operatorname{rank}(\mathbf{X}) = d $, each row a transposed data example
$ \mathbf{x}_k^\top $, centered response vector
$ \mathbf{y} \in \mathbb{R}^N $, $ \lambda > 0 $, the ridge regression problem
can be written in matrix-vector form as
\begin{equation*}
    \begin{array}{ll}
        \displaystyle\min_\mathbf{w} &
        (\mathbf{y} - \mathbf{Xw})^\top(\mathbf{y} - \mathbf{Xw}) +
        \lambda\Vert\mathbf{w}\Vert_2^2
    \end{array}
\end{equation*}

Find an analytical expression for $ \tilde{\mathbf{w}} \in \mathbb{R}^d $, the
\textit{ridge estimator}, that solves this problem.

\medskip

\textit{Hints.} The objective is convex, so its gradient is $ \mathbf{0} $ at
$ \tilde{\mathbf{w}} $. $ \mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I} $ is
invertible.

\bibliographystyle{acm}
\bibliography{../master_bib}

\end{document}