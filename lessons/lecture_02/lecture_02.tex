% standard beamer lecture template for slides
% by Derek Huang
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e, amsmath, amssymb, amsfonts, graphicx}
% allow section.equation numbering
\numberwithin{equation}{section}
% use boadilla theme
\usetheme{Boadilla}
% remove navigation symbols
\usenavigationsymbolstemplate{}
% get numbered figure captions
\setbeamertemplate{caption}[numbered]
% changes itemize to circle + other things
\useoutertheme{split}
\useinnertheme{circles}

% title page stuff. brackets content displayed in footer bar
\title[Math for Data Science, Part 2]{Math for Data Science, Part 2}
% metadata. content in brackets is displayed in footer bar
\author[Derek Huang (BAC Advanced Team)]{Derek Huang}
\institute{BAC Advanced Team}
\date{February 25, 2021}

% change "ball" bullet to numbered bullet and section title for section
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
% change ball to gray square (copied from stackoverflow; \par needed for break)
\setbeamertemplate{subsection in toc}{        
    \hspace{1.2em}{\color{gray}\rule[0.3ex]{3pt}{3pt}}~\inserttocsubsection\par}
% use default enumeration scheme
\setbeamertemplate{enumerate items}[default]
% required line that fixes the problem of \mathbf, \bf not working in beamer
% for later (post-2019) TeX Live installations. see the issue on GitHub:
% https://github.com/josephwright/beamer/issues/630
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{}

\begin{document}

% title slide
\begin{frame}
    \titlepage
    \centering
    \includegraphics[scale = 0.1]{../bac_logo1.png}
\end{frame}

% table of contents slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

\section{Linear algebra}

\subsection{Matrix essentials}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{matrix} $ \mathbf{A} $ of shape
        $ m \times n $ over a field\footnotemark\footnotetext{
            It turns out that it doesn't matter if $ \mathbb{F} = \mathbb{R} $
            or $ \mathbb{F} = \mathbb{C} $, $ \mathbb{C} $ the complex numbers
            \cite{jacob_linalg}.
        } $ \mathbb{F} $ is a collection of indexed elements of $ \mathbb{F} $
        arranged in a rectangular array. The $ i, j$th element of
        $ \mathbf{A} $ is denoted as $ A_{ij} $ or $ a_{ij} $.
        \begin{equation*}
            \mathbf{A} = \begin{bmatrix}
                \ a_{11} & \ldots & a_{1n} \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ a_{m1} & \ldots & a_{mn} \
            \end{bmatrix}
        \end{equation*}

        \item
        \textit{Definition.} A \textit{column vector} $ \mathbf{a} $ with
        $ n $ elements is a $ n \times 1 $ matrix over a field $ \mathbb{F} $
        with elements denoted as $ a_1, \ldots a_n $.
        \begin{equation*}
            \mathbf{a} = \begin{bmatrix}
                \ a_1 \ \\ \ \vdots \ \\ \ a_n \
            \end{bmatrix}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        \textit{Definition.} The $ n \times n $ \textit{identity matrix}
        $ \mathbf{I} $ is such that
        \begin{equation*}
            \mathbf{I} \triangleq \begin{bmatrix}
                \ 1 & \ldots & 0 \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ 0 & \ldots & 1 \
            \end{bmatrix}
        \end{equation*}
        If context is not clear, we write $ n \times n $ identity as
        $ \mathbf{I}_n $.

	    \item
        \textit{Definition.} The \textit{transpose} of a matrix
        $ \mathbf{A} \in \mathbb{R}^{m \times n } $ is $ \mathbf{A}^\top
        \in \mathbb{R}^{n \times m} $, where the $ i, j $th element of
        $ \mathbf{A}^\top $ is the $ j, i $th element of $ \mathbf{A} $.
        \begin{equation*}
            \mathbf{A}^\top \triangleq \begin{bmatrix}
                \ a_{11} & \ldots & a_{m1} \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ a_{1n} & \ldots & a_{mn} \
            \end{bmatrix}        
        \end{equation*}

        \item
        \textit{Theorem.} For $ \mathbf{A} \in \mathbb{F}^{m \times p} $,
        $ \mathbf{B} \in \mathbb{F}^{p \times n} $,
        $ (\mathbf{AB})^\top = \mathbf{B}^\top\mathbf{A}^\top $
        \cite{jacob_linalg}.
    \end{itemize}
\end{frame}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        \textit{Definition.} The \textit{sum} of matrices 
        $ \mathbf{A}, \mathbf{B} \in \mathbb{F}^{m \times n }$ is such that
        \begin{equation*}
            \mathbf{A} + \mathbf{B} = \begin{bmatrix}
                \ a_{11} + b_{11} & \ldots & a_{1n} + b_{1n} \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ a_{m1} + b_{m1} & \ldots & a_{mn} + b_{mn} \
            \end{bmatrix}
        \end{equation*}

        \item
        \textit{Properties of matrix addition} \cite{jacob_linalg}. For
        $ \mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{F}^{m \times n} $,
        $ a, b \in \mathbb{F} $
        \begin{enumerate}
            \item
            $ \mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A} $
            (commutative).

            \item
            $ (\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} +
            (\mathbf{B} + \mathbf{C}) $ (associative).

            \item
            $ a(\mathbf{A} + \mathbf{B}) = a\mathbf{A} + a\mathbf{B} $ (scalar
            distribution).

            \item
            $ (a + b)\mathbf{A} = a\mathbf{A} + b\mathbf{A} $ (scalar
            distribution).

            \item
            $ a(b\mathbf{A}) = (ab)\mathbf{A} $ (scalar associativity).
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        \textit{Definition.} The \textit{product} of $ \mathbf{A} \in
        \mathbb{F}^{m \times d} $, $ \mathbf{B} \in \mathbb{F}^{d \times n} $
        is such that
        \begin{equation*}
            \mathbf{AB} = \begin{bmatrix}
                \ \sum_{k = 1}^da_{1k}b_{k1} & \ldots &
                    \sum_{k = 1}^da_{1k}b_{kn} \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ \sum_{k = 1}^da_{mk}b_{k1} & \ldots &
                    \sum_{k = 1}^da_{mk}b_{kn} \
            \end{bmatrix} \in \mathbb{F}^{m \times n}
        \end{equation*}

        \item
	    \textit{Properties of matrix multiplication} \cite{jacob_linalg}. For
	    matrices $ \mathbf{A}, \mathbf{B} \in \mathbb{F}^{m \times d} $,
	    $ \mathbf{C}, \mathbf{D} \in \mathbb{F}^{d \times n} $, and a scalar
	    $ a \in \mathbb{F} $,
	    \begin{enumerate}
	        \item
            $ a(\mathbf{AC}) = (a\mathbf{A})\mathbf{C} $ (scalar
            associativity).

            \item
            $ a(\mathbf{AC}) = \mathbf{A}(a\mathbf{C}) $ (scalar
            commutativity + associativity).

            \item
            $ (\mathbf{A} + \mathbf{B})\mathbf{C} =
            \mathbf{AC} + \mathbf{BC} $ (matrix distribution).

            \item
            $ \mathbf{AI}_d = \mathbf{A} $, $ \mathbf{I}_d\mathbf{C} =
            \mathbf{C} $ (identity).
	    \end{enumerate}

        \item
        \textit{Theorem.} For $ \mathbf{A} \in \mathbb{F}^{m \times p} $,
        $ \mathbf{B} \in \mathbb{F}^{p \times q} $, $ \mathbf{C} \in
        \mathbb{F}^{q \times n} $, $ (\mathbf{AB})\mathbf{C} = 
        \mathbf{A}(\mathbf{BC}) $ \cite{jacob_linalg}.
    \end{itemize}
\end{frame}

%\begin{frame}{Matrix essentials}
%    \begin{itemize}
%        \item
%        \textit{Definition.} Given matrices $ \mathbf{A}, \mathbf{B} \in
%        \mathbb{F}^{m \times n} $, their \textit{Hadamard product} is
%        \begin{equation*}
%            \mathbf{A} \odot \mathbf{B} \triangleq \begin{bmatrix}
%                \ a_{11}b_{11} & \ldots & a_{1n}b_{1n} \ \\
%                \ \vdots & \ddots & \vdots \ \\
%                \ a_{m1}b_{m1} & \ldots & a_{mn}b_{mn} \
%            \end{bmatrix}
%        \end{equation*}
%
%        \item
%        \textit{Definition.} Given matrices $ \mathbf{A}, \mathbf{B} \in
%        \mathbb{F}^{m \times n} $, \textit{Hadamard division} is
%        \begin{equation*}
%            \mathbf{A} \oslash \mathbf{B} \triangleq \begin{bmatrix}
%                \ a_{11} / b_{11} & \ldots & a_{1n} / b_{1n} \ \\
%                \ \vdots & \ddots & \vdots \ \\
%                \ a_{m1} / b_{m1} & \ldots & a_{mn} / b_{mn} \
%            \end{bmatrix}
%        \end{equation*}
%
%        \item
%        \textit{Definition.} Given matrix $ \mathbf{A} \in
%        \mathbb{F}^{m \times n} $, $ b \in \mathbb{R} $, the
%        \textit{Hadamard power} is
%        \begin{equation*}
%            \mathbf{A}^{\odot b} \triangleq \begin{bmatrix}
%                \ a_{11}^b & \ldots & a_{1n}^b \ \\
%                \ \vdots & \ddots & \vdots \ \\
%                \ a_{m1}^n & \ldots & a_{mn}^b \
%            \end{bmatrix}
%        \end{equation*}
%    \end{itemize}
%\end{frame}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        \textit{Definition.} $ \mathbf{A} \in \mathbb{F}^{m \times n} $
        is \textit{diagonal} if $ a_{11}, \ldots a_{qq}  \in \mathbb{F} $,
        $ q \triangleq \min\{m, n\} $ are the only nonzero elements in
        $ \mathbf{A} $. If $ \mathbf{A} \in \mathbb{F}^{n \times n} $, i.e.
        square, for $ a_1, \ldots a_n \in \mathbb{F} $, $ \mathbf{A} $ can
        be written as $ \operatorname{diag}(a_1, \ldots a_n) $.
        
        \item
        \textit{Definition.} If $ \mathbf{A} \in \mathbb{F}^{n \times n} $ is
        \textit{invertible}, $ \exists! \mathbf{A}^{-1} \in
        \mathbb{F}^{n \times n} $ such that
        \begin{equation*}
            \mathbf{A}^{-1}\mathbf{A} = \mathbf{AA}^{-1} = \mathbf{I}
        \end{equation*}
        The $ \exists! $ notation means ``uniquely exists''.

        \item
        \textit{
            Some properties of invertible matrices\footnotemark\footnotetext{
                We'll discuss rank later in the slides. Don't worry about
                eigenvalues for now.
            }.
        } If $ \mathbf{A} \in
        \mathbb{F}^{n \times n} $ invertible,
        \begin{itemize}
            \item
            $ |\mathbf{A}| \ne 0 $ (nonzero determinant).

            \item
            $ \forall \mathbf{b} \in \mathbb{F}^n $, $ \exists! \mathbf{x} \in
            \mathbb{F}^n $ s.t. $ \mathbf{Ax} = \mathbf{b} $ (unique solution
            to linear system).

            \item
            $ \operatorname{rk}(\mathbf{A}) = n $ (full rank).

            \item
            All eigenvalues of $ \mathbf{A} $ are nonzero.
        \end{itemize}

        \item
        \textit{Theorem.} For invertible $ \mathbf{A}, \mathbf{B} \in
        \mathbb{F}^{n \times n} $, $ (\mathbf{AB})^{-1} =
        \mathbf{B}^{-1}\mathbf{A}^{-1} $ \cite{jacob_linalg}.

        \item
        \textit{Theorem.} For $ \mathbf{A} \triangleq
        \operatorname{diag}(a_1, \ldots a_n) $, $ \mathbf{A}^{-1} \triangleq
        \operatorname{diag}\left(a_1^{-1}, \ldots a_n^{-1}\right) $.
    \end{itemize}

    % skip for footnote spacing
    \medskip
\end{frame}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        \textit{Definition.} $ \mathbf{A} \in \mathbb{R}^{n \times n} $ is
        \textit{symmetric} if $ \mathbf{A} = \mathbf{A}^\top $.

        \item
        \textit{Definition.} Symmetric $ \mathbf{A} \in
        \mathbb{R}^{n \times n} $ is \textit{positive definite} if
        $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \mathbf{x}^\top
        \mathbf{Ax} > 0 $. $ \mathbf{A} $ is \textit{positive semidefinite}
        if $ \forall \mathbf{x} \in \mathbb{R}^n,
        \mathbf{x}^\top\mathbf{Ax} \ge 0 $. $ \mathbf{A} \succ \mathbf{0} $
        means $ \mathbf{A} $ positive definite, while $ \mathbf{A}
        \succeq \mathbf{0} $ means $ \mathbf{A} $ positive semidefinite.

        \item
        \textit{Some properties of positive definite matrices.} For
        $ \mathbf{A} \succ \mathbf{0} $,
        \begin{enumerate}
            \item
            $ \exists \mathbf{A}^{-1} \succ \mathbf{0} $ (positive definite
            inverse).

            \item
            All eigenvalues are real and positive.

            \item
            $ |\mathbf{A}| > 0 $ (positive determinant).
        \end{enumerate}

        \item
        \textit{Remark.} For $ \mathbf{A} \in \mathbb{R}^{n \times n} $,
        $ \mathbf{A} \succeq \mathbf{0} $ does \textbf{not} mean that all
        elements of $ \mathbf{A} $ are nonnegative. For example,
        $ \mathbf{Q} \triangleq \mathbf{I}_n - \frac{1}{n^2}\mathbf{11}^\top
        \succ \mathbf{0} $, as $ |\mathbf{Q}| > 0 $.

        \item
        \textit{Some properties of positive semidefinite matrices.} For
        $ \mathbf{A} \succeq \mathbf{0} $,
        \begin{enumerate}
            \item
            $ \exists \mathbf{A}^{-1} \succ \mathbf{0} \Leftrightarrow $ all
            eigenvalues positive.

            \item
            All eigenvalues are real and nonnegative.

            \item
            $ |\mathbf{A}| \ge 0 $ (nonnegative determinant).
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}{Matrix essentials}
    \begin{itemize}
        \item
        Matrices and vectors are very useful for applied modeling.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Representing data.} Given data points $ \mathbf{x}_1,
            \ldots \mathbf{x}_N \in \mathbb{R}^d $, we can define the matrix
            $ \mathbf{X} \in \mathbb{R}^{N \times d} $ where each $ k $th row
            is $ \mathbf{x}_k^\top $.

            \item
            \textit{Linear dynamical system.} Suppose $ \mathbf{x}_k \in
            \mathbb{R}^n $ represents the state of a system at time step
            $ k \in \mathbb{N} $. Suppose system state evolution depends only
            on fixed linear transformation of $ \mathbf{x}_k $ and
            independent noise. Then,
            \begin{equation*}
                \mathbf{x}_{k + 1} = \mathbf{Qx}_k + \mathbf{z}_k
            \end{equation*}
            Here $ \mathbf{Q} \in \mathbb{R}^{n \times n} $, $ \mathbf{z}_k $
            i.i.d. where $ \mathbb{E}[\mathbf{z}_k] = \mathbf{0} $,
            $ \mathbf{C}_\mathbf{z} = \operatorname{diag}(\sigma_1, \ldots
            \sigma_n) \succ \mathbf{0} $.

            \item
            \textit{Multivariate quadratic function.} Given symmetric
            $ \mathbf{Q} \in \mathbb{R}^{n \times n} $, $ \mathbf{a} \in
            \mathbb{R}^n $, $ b \in \mathbb{R} $, a quadratic function
            $ f : \mathbb{R}^n \rightarrow \mathbb{R} $ can be represented
            such that
            \begin{equation*}
                f(\mathbf{x}) \triangleq \mathbf{x}^\top\mathbf{Qx} +
                \mathbf{a}^\top\mathbf{x} + b
            \end{equation*}
            $ \mathbf{Q} $ gives the quadratic coefficients, $ \mathbf{a} $
            the linear coefficients.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Vector spaces}

\begin{frame}{Vector spaces}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{field} $ \mathbb{F} $ is a set equipped
        with addition and multiplication with elements $ 0, 1 \in \mathbb{F} $
        s.t. $ \forall x, y, z \in \mathbb{F} $ \cite{jacob_linalg},
        \begin{enumerate}
            \item
            $ x + y \in \mathbb{F} $ (closure under addition).

            \item
            $ x + y = y + x $ (commutativity of addition).

            \item
            $ (x + y) + z = x + (y + z) $ (associativity of addition).

            \item
            $ 0 + x = x $ (contains additive identity).

            \item
            $ xy \in \mathbb{F} $ (closure under multiplication).

            \item
            $ xy = yx $ (commutativity of multiplication).

            \item
            $ (xy)z = x(yz) $ (associativity of multiplication).

            \item
            $ 1x = x $ (contains multiplicative identity).

            \item
            $ x(y + z) = xy + xz $ (distribution of multiplication).

            \item
            $ \exists!x' \in \mathbb{F} $ s.t. $ x + x' = 0 $ (unique additive
            inverses).

            \item
            If $ x \ne 0 $, $ \exists!\tilde{x} \in \mathbb{F} $ s.t.
            $ x\tilde{x} = 1 $ (unique multiplicative inverses).
        \end{enumerate}

        \item
        \textit{Remark.} We will use $ \mathbb{F} $ to denote arbitrary fields.

        \item
        \textit{Examples.} $ \mathbb{Q} $, $ \mathbb{R} $, $ \mathbb{C} $. We
        focus mostly on $ \mathbb{R} $.
    \end{itemize}
\end{frame}

\begin{frame}{Vector spaces}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{vector space} $ V $ over a field
        $ \mathbb{F} $ is a set of \textit{vectors} containing the
        \textit{zero vector} $ 0 \in V $ equipped with \textit{vector
        addition} and \textit{scalar multiplication} of $ v \in V $ by
        $ x \in \mathbb{F} $. $ \forall u, v, w \in V $, $ \forall \alpha,
        \beta \in \mathbb{F} $ \cite{jacob_linalg},
        \begin{enumerate}
            \item
            $ u + v \in V $ (closure under vector addition).

            \item
            $ u + v = v + u $ (commutativity of vector addition).

            \item
            $ 0 + u = u $ (contains vector addition identity).

            \item
            $ (u + v) + w = u + (v + w) $ (associativity of vector addition).

            \item
            $ \alpha u \in V $ (closure under scalar multiplication).

            \item
            $ 1u = u $, $ 1 \in \mathbb{F} $ ($ \mathbb{F} $ has scalar
            multiplicative identity).

            \item
            $ \alpha(u + v) = \alpha u + \alpha v $ (distribution of scalar
            multiplication).

            \item
            $ (\alpha + \beta)u  = \alpha u + \beta u $ (distribution of
            scalar multiplication).

            \item
            $ (\alpha\beta)u = u(\alpha\beta) $ (commutativity of scalar
            multiplication).
        \end{enumerate}

        \item
        \textit{Remark.} These properties should be natural; no need to
        memorize.

        \item
        \textit{Example.} $ \mathbb{R}^n $, i.e. $ n $\textit{-dimensional
        Euclidean space}. For any field $ \mathbb{F} $, $ \mathbb{F}^n $ is
        the $ n $\textit{-fold Cartesian product}, for example
        $ \mathbb{C}^n $.

        \item
        \textit{Remark.} We will denote elements of $ \mathbb{R}^n $ in bold.
    \end{itemize}
\end{frame}

\begin{frame}{Vector spaces}
    \begin{itemize}
        \item
        \textit{Definition.} For $ v_1, \ldots v_n \in V $, $ V $ a vector
        space over $ \mathbb{F} $, and $ a_1, \ldots a_n \in \mathbb{F} $,
        $ w = \sum_{k = 1}^na_kv_k $ is a \textit{linear combination}.

        \item
        \textit{Definition.} The \textit{span} of $ v_1, \ldots v_n \in V $,
        denoted $ \operatorname{span}\{v_1, \ldots v_n\} $, is the set of all
        linear combinations of $ v_1, \ldots v_n $ \cite{jacob_linalg}.

        \item
        \textit{Definition.} $ v_1, \ldots v_n \in V $ are \textit{linearly
        independent} if $ \sum_{k = 1}^na_kv_k = 0 \in V \Leftrightarrow
        a_1 = \ldots a_n = 0 \in \mathbb{F} $, else \textit{linearly
        dependent}.

        \item
        \textit{Definition.} A set $ \{v_1, \ldots v_n\} \subset V $ is called
        a \textit{basis} for $ V $ if \cite{jacob_linalg}
        \begin{enumerate}
            \item
            $ v_1, \ldots v_n $ are linearly independent.

            \item
            $ V = \operatorname{span}\{v_1, \ldots v_n\} $.
        \end{enumerate}

        \item
        \textit{Remark.} Although unordered, bases are often treated as
        ordered.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{The standard Euclidean basis.} The standard basis for
            $ \mathbb{R}^n $ is $ \{\mathbf{e}_1, \ldots \mathbf{e}_n\} $,
            where the $ k $th element of $ \mathbf{e}_k $ is $ 1 $ and all
            other elements are zero.

            \item
            \textit{Non-standard} $ \mathbb{R}^2 $ \textit{basis.}
            $ \left\{[ \ -1 \ \ 1 \ ]^\top, [ \ 2 \ \ 0 \ ]^\top\right\} $ is
            a basis for $ \mathbb{R}^2 $.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Vector spaces}
    \begin{itemize}
        \item
        \textit{Lemma.} If $ \{v_1, \ldots v_n\} \subset V $,
        $ \{w_1, \ldots w_m\} \subset V $ are both finite bases of a vector
        space $ V $, then $ m = n $ \cite{jacob_linalg}.

        \item
        \textit{Definition.} If vector space $ V $ has a basis $ B \subset V $
        where $ |B| = n $, $ n \in \mathbb{N} $, then $ V $ is said to be
        $ n $\textit{-dimensional}.

        \item
        \textit{Remark.} $ \mathbb{F}^n $ is often referred to as
        $ n $-dimensional, but in general, for a vector space $ V $ on field
        $ \mathbb{F} $, dimension $ \ne $ number of elements of $ \mathbb{F} $
        in a vector $ v \in V $. A vector \textit{subspace} $ U \subset
        \mathbb{R}^n $ that has $ k < n $ elements in its basis has dimension
        $ k $ although $ \mathbf{u} \in U $ contains $ n $ elements of
        $ \mathbb{R} $.

        \item
        \textit{Definition.} Given ordered basis $ B \subset \mathbb{F}^n $,
        denote $ \mathbf{B} \in \mathbb{F}^{n \times n} $ as the matrix whose
        columns hold the elements of $ B $. For $ \mathbf{x} \in
        \mathbb{F}^n $, its \textit{coordinates} in $ B $, denoted by
        $ (\mathbf{x})_B $ are such that
        $ \mathbf{x} = \mathbf{B}(\mathbf{x})_B $.

        \item
        \textit{Theorem.} $ \mathbf{B} $ is invertible and
        $ \forall \mathbf{u} \in \mathbb{F}^n $,
        $ (\mathbf{u})_B = \mathbf{B}^{-1}\mathbf{u} $ \cite{jacob_linalg}.

        \item
        \textit{Remark.} Bases and coordinates are important in the context of
        principal components analysis, which performs a \textit{change of
        basis}.
    \end{itemize}
\end{frame}

\begin{frame}{Vector spaces}
    \begin{itemize}
        \item
        \textit{Definition.} The \textit{rank} of $ \mathbf{A} \in
        \mathbb{F}^{m \times n} $ is the maximal number of linearly
        independent columns/rows of $ \mathbf{A} $, denoted as
        $ \operatorname{rk}(\mathbf{A}) $ or
        $ \operatorname{rank}(\mathbf{A}) $.

        \item
        \textit{Definition.} $ \mathbf{A} \in \mathbb{R}^{m \times n} $ is
        \textit{full rank} if $ \operatorname{rk}(\mathbf{A}) = \min\{m, n\} $.

        \item
        Rank can be thought of as a matrix's ``information content''.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Feature redundancy.} Suppose $ \mathbf{X} \in
            \mathbb{R}^{N \times d} $, each $ k $th row a transposed data
            example $ \mathbf{x}_k^\top $. Assuming $ N > d $, if
            $ \operatorname{rk}(\mathbf{X}) < d $, at least one column is
            linearly dependent and can be safely removed.

            \item
            \textit{Square linear system.} Suppose $ \mathbf{A} \in
            \mathbb{R}^{n \times n} $, $ \mathbf{b} \in \mathbb{R}^n $. We
            want to find $ \mathbf{x} \in \mathbb{R}^n $ s.t. $ \mathbf{Ax} =
            \mathbf{b} $. If $ \operatorname{rk}(\mathbf{A}) = n $, each row
            (linear equation) helps to determine one of the $ n $ unknown
            elements of $ \mathbf{x} $. If
            $ \operatorname{rk}(\mathbf{A}) < n $, there isn't enough
            information $ \Rightarrow  $ no solution or infinitely many
            solutions.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Norms and inner products}

\begin{frame}{Norms and inner products}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{norm} on a vector space $ V $ over
        $ \mathbb{F} \in \{\mathbb{R}, \mathbb{C}\} $ is a function
        $ \Vert\cdot\Vert : V \rightarrow \mathbb{R} $ that satisfies the
        following properties \cite{jacob_linalg}:
        \begin{enumerate}
            \item
            $ \forall v \in V $, $ \Vert v\Vert \ge 0 $, where
            $ \Vert v\Vert = 0 \Leftrightarrow v = 0 $.

            \item
            $ \forall k \in \mathbb{F} $, $ \forall v \in V $,
            $ \Vert kv\Vert = |k|\Vert v\Vert $ (absolute homogeneity).

            \item
            $ \forall u, v \in V $, $ \Vert u + v\Vert \le
            \Vert u\Vert + \Vert v\Vert $ (triangle inequality).
        \end{enumerate}

        \item
        A norm gives a sense of the ``size'' of a vector in a vector space.

        \item
        \textit{Definition.} $ \Vert\mathbf{x}\Vert_p $, the finite
        $ \ell^p $\textit{-norm}, or $ p $\textit{-norm}, of
        $ \mathbf{x} \in \mathbb{R}^n $ is s.t.
        \begin{equation*}
            \Vert\mathbf{x}\Vert_p \triangleq
            \left(\sum_{k = 1}^n|x_k|^p\right)^{1 / p}
        \end{equation*}
        When $ p = 2 $, $ \Vert\cdot\Vert_2 $ called the \textit{Euclidean},
        or $ \ell^2 $ norm. When $ p = 1 $, $ \Vert\cdot\Vert_1 $ called the
        $ \ell^1 $, \textit{Manhattan}, or \textit{taxicab} norm. When
        $ p = \infty $, $ \Vert\mathbf{x}\Vert_\infty \triangleq
        \max\{|x_1|, \ldots |x_n|\} $, and $ \Vert\cdot\Vert_\infty $ called
        the \textit{max norm}.
    \end{itemize}
\end{frame}

\begin{frame}{Norms and inner products}
    \begin{itemize}
        \item
        \textit{Definition.} An \textit{inner product} on a vector space $ V $
        over $ \mathbb{R} $ is a function $ \langle\cdot, \cdot\rangle :
        V \times V \rightarrow \mathbb{R} $ that satisfies the following
        properties \cite{jacob_linalg}:
        \begin{enumerate}
            \item
            $ \forall u, v \in V $, $ \langle u, v\rangle =
            \langle v, u\rangle $ (symmetric).

            \item
            $ \forall k \in \mathbb{R} $, $ \forall u, v \in V $,
            $ \langle ku, v\rangle = k\langle u, v\rangle $ (bilinear).

            \item
            $ \forall u, v, w \in V $, $ \langle u + v, w\rangle =
            \langle u, w\rangle + \langle v, w\rangle $ (bilinear).

            \item
            $ \forall v \in V $, $ \langle v, v\rangle \ge 0 $, where
            $ \langle v, v\rangle = 0 \Leftrightarrow v = 0 $.
        \end{enumerate}

        \item
        \textit{Definition.} A vector space $ V $ over $ \mathbb{R} $ equipped
        with an inner product $ \langle\cdot, \cdot\rangle $ is called a
        \textit{real inner product space} \cite{jacob_linalg}.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            $ \mathbb{R}^n $. $ \mathbb{R}^n $ equipped with the dot product
            $ \cdot $, where $ \forall \mathbf{u}, \mathbf{v} \in
            \mathbb{R}^n $, $ \mathbf{u}\cdot\mathbf{v} =
            \mathbf{u}^\top\mathbf{v} $, is a finite-dimensional inner product
            space.

            \item
            $ L^2(\mathbb{R}) $. The $ L^2 $ function space on $ \mathbb{R} $,
            i.e. the vector space of all functions $ f : \mathbb{R}
            \rightarrow \mathbb{R} $ s.t.
            $ \int_\mathbb{R}f(x)^2\,dx < \infty $ where $ \forall f, g
            \in L^2(\mathbb{R}) $,
            \begin{equation*}
                \langle f, g\rangle \triangleq \int_\mathbb{R}f(x)g(x)\,dx
            \end{equation*}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Norms and inner products}
    \begin{itemize}
        \item
        Inner products can be thought of as measuring vector similarity.

        \item
        \textit{Theorem.} If $ \langle\cdot, \cdot\rangle : V \times V
        \rightarrow \mathbb{R} $ is an inner product on real vector $ V $
        space, i.e. a vector space over $ \mathbb{R} $, then
        $ \Vert\cdot\Vert : V \rightarrow \mathbb{R} $ where
        $ \forall v \in V $, $ \Vert v\Vert \triangleq
        \sqrt{\langle v, v\rangle} $, is a norm on $ V $ \cite{jacob_linalg}.
        \begin{itemize}
            \item
            In particular, if $ V = \mathbb{R}^n $, $ \forall \mathbf{u},
            \mathbf{v} \in \mathbb{R}^n $, $ \langle\mathbf{u},
            \mathbf{v}\rangle  = \mathbf{u}^\top\mathbf{v} $,
            $ \Vert\mathbf{u}\Vert_2 = \sqrt{\mathbf{u}^\top\mathbf{u}} $.
        \end{itemize}

        \item
        \textit{Definition.} $ u \in V $ is a \textit{unit vector} if
        $ \Vert u\Vert = 1 $.        

        \item
	    \textit{Definition.} $ u, v \in V $ are \textit{orthogonal} if 
	    $ \langle u, v\rangle = 0 $, denoted as $ u \perp v $.

        \item
        \textit{Definition.} $ u, v \in V $ are \textit{orthonormal} if
        $ \langle u, v\rangle = 0 $, $ \Vert u\Vert = \Vert v\Vert = 1 $.

        \item
        \textit{Theorem.} $ \forall \mathbf{u}, \mathbf{v} \in \mathbb{R}^n $,
        $ \mathbf{u}^\top\mathbf{v} = \Vert\mathbf{u}\Vert_2
        \Vert\mathbf{v}\Vert_2\cos\theta $, $ \theta \in [0, \pi] $.
        $ \theta $ is the angle between $ \mathbf{u}, \mathbf{v} $, giving the
        similarity of $ \mathbf{u}, \mathbf{v} $ in direction.

        \item
        \textit{Corollary.} $ \theta = 0 \Leftrightarrow
        \mathbf{u} = k\mathbf{v} $, $ k \in (0, \infty) $, and $ \mathbf{u} =
        \mathbf{v} \Rightarrow \theta = 0 $.

        \item
        \textit{Corollary.} $ \mathbf{u}^\top\mathbf{v} = 0 \Leftrightarrow
        \theta = \pi / 2 $.
    \end{itemize}
\end{frame}

\begin{frame}{Norms and inner products}
    \begin{itemize}
        \item
        \textit{Definition.} A basis $ B \triangleq \{v_1, \ldots v_n\} \subset
        V $ is called \textit{orthogonal}
        if $ \forall v_i, v_j $, $ i \ne j $, $ i, j \in \{1, \ldots n\} $,
        $ \langle v_i, v_j\rangle = 0 $.

        \item
        \textit{Definition.} An orthogonal basis $ B \triangleq
        \{v_1, \ldots v_n\} \subset V $, where $ \Vert v\Vert = 1 $,
        $ k \in \{1, \ldots n\} $, is called an \textit{orthonormal basis}.

        \item
        \textit{Definition.} $ \mathbf{Q} \in \mathbb{R}^{n \times n} $ is
        \textit{orthogonal}\footnotemark\footnotetext{
            Should really be called orthonormal; this is a misnomer.        
        } if $ \mathbf{Q}^\top\mathbf{Q} = \mathbf{QQ}^\top
        = \mathbf{I} $. That is, the rows and columns of $ \mathbf{Q} $ are
        orthonormal.

        \item
        \textit{Definition.} $ \mathbf{Q} \in \mathbb{R}^{m \times n} $ has
        \textit{orthonormal columns} if $ \mathbf{Q}^\top\mathbf{Q} =
        \mathbf{I}_n $ and \textit{orthonormal rows} if $ \mathbf{QQ}^\top =
        \mathbf{I}_m $.

        \item
        \textit{Remark.} Orthogonal matrices have appealing analytical and
        numerical properties. For example, linear ordinary least squares
        problems are often solved in practice using the singular value
        decomposition\footnotemark\footnotetext{
            \href{
https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html
            }{\alert{\texttt{scipy.linalg.lstsq}}} wraps LAPACK's
            \href{
            http://www.netlib.org/lapack/explore-html/db/d6a/dgelsd_8f.html
            }{\alert{\texttt{gelsd}}} routine.
        }.
    \end{itemize}
\end{frame}

\section{Vector calculus}

\subsection{Gradient, Jacobian, Hessian}

\begin{frame}{Gradient, Jacobian, Hessian}
    \begin{itemize}
        \item
        \textit{Definition.} For $ f : \mathbb{R}^n \rightarrow \mathbb{R} $,
        the \textit{gradient} $ \nabla f : \mathbb{R}^n \rightarrow
        \mathbb{R}^n $ is such that
        \begin{equation*}
            \nabla f(\mathbf{x}') \triangleq \begin{bmatrix}
                \ \frac{\partial f}{\partial x_1}(\mathbf{x}') \ \\
                \ \vdots \ \\
                \ \frac{\partial f}{\partial x_n}(\mathbf{x}') \
            \end{bmatrix} \triangleq \left[
                \frac{\partial f}{\partial \mathbf{x}}(\mathbf{x}')
            \right]^\top
        \end{equation*}
        Here $ \mathbf{x}' \in \mathbb{R}^n $ and
        $ \frac{\partial f}{\partial \mathbf{x}} : \mathbb{R}^n \rightarrow
        \mathbb{R}^{1 \times n} $ is the \textit{vector
        derivative}\footnotemark\footnotetext{
            The fact that this is a \textbf{row} vector is important in matrix
            calculus.        
        }.

        \item
        \textit{Definition.} For $ \mathbf{f} : \mathbb{R}^n \rightarrow
        \mathbb{R}^m $, its \textit{Jacobian} is
        \begin{equation*}
            \nabla\mathbf{f}(\mathbf{x}') \triangleq \begin{bmatrix}
                \ \frac{\partial f_1}{\partial x_1}(\mathbf{x}') \ & \ldots &
                    \frac{\partial f_1}{\partial x_n}(\mathbf{x}') \ \\
                \ \vdots & \ddots & \vdots \ \\
                \frac{\partial f_m}{\partial x_1}(\mathbf{x}') \ & \ldots &
                    \frac{\partial f_m}{\partial x_n}(\mathbf{x}') \
            \end{bmatrix} \triangleq \begin{bmatrix}
                \ \frac{\partial f_1}{\partial\mathbf{x}}(\mathbf{x}') \ \\
                \ \vdots \ \\
                \ \frac{\partial f_m}{\partial\mathbf{x}}(\mathbf{x}') \
            \end{bmatrix} \in \mathbb{R}^{m \times n}
        \end{equation*}
        Here $ \mathbf{x}' \in \mathbb{R}^n $, $ \mathbf{f} \triangleq
        [ \ f_1 \ \ldots \ f_ m \ ]^\top $, $ f_k : \mathbb{R}^n
        \rightarrow \mathbb{R} $, $ \forall k \in \{1, \ldots n\} $.
    \end{itemize}
    
    % adjust spacing for footnote
    \bigskip
\end{frame}

\begin{frame}{Gradient, Jacobian, Hessian}
    \begin{itemize}
        \item
        Gradient is linear approximation of function change along axes.

        \item
        \textit{Definition.} For $ f : \mathbb{R}^n \rightarrow \mathbb{R} $,
        the \textit{Hessian} $ \nabla^2f : \mathbb{R}^n \rightarrow
        \mathbb{R}^{n \times n} $ is s.t.
        \begin{equation*}
            \nabla^2 f(\mathbf{x}) \triangleq \begin{bmatrix}
                \ \frac{\partial^2 f}{\partial x_1^2}(\mathbf{x}) & \ldots &
                    \frac{\partial^2 f}{\partial x_1x_n}(\mathbf{x}) \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ \frac{\partial^2 f}{\partial x_nx_1}(\mathbf{x}) & \ldots &
                    \frac{\partial^2 f}{\partial x_n^2}(\mathbf{x}) \
            \end{bmatrix}
        \end{equation*}
        $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \nabla^2f(\mathbf{x})
        = \nabla^2f(\mathbf{x})^\top $ ($ \nabla^2f(\mathbf{x}) $ symmetric).

        \item
        The Hessian approximates the local curvature of $ f $ at a point.

        \item
        Provides a way to check if $ f $ is locally \textit{convex} at a
        point. If $ f $ is globally convex, then $ \forall \mathbf{x} \in
        \operatorname{dom}f $, $ \nabla^2f(\mathbf{x}) \succeq \mathbf{0} $
        \cite{bv_convex_opt}.

        \item
        For large $ n $, computing and storing the Hessian is expensive, so
        in practice repeatedly computing the Hessian is not a good idea.
    \end{itemize}
\end{frame}

\subsection{Derivative cookbook}

\begin{frame}{Derivative cookbook}
    \begin{itemize}
        \item
	    Gradients, Jacobians, and Hessians of some common expressions.
	
	    \item
	    \textit{Squared} $ \ell^2 $\textit{-norm.} For $ \mathbf{x} \in
	    \mathbb{R}^n $, $ \nabla \Vert\mathbf{x}\Vert_2^2 =
	    \nabla\mathbf{x}^\top\mathbf{x} = 2\mathbf{x} $.
	    \begin{equation*}
            \mathbf{x}^\top\mathbf{x} = \sum_{k = 1}^nx_k^2 \Rightarrow
            \nabla \mathbf{x}^\top\mathbf{x} = \begin{bmatrix}
                \ 2x_1 \ \\ \ \vdots \ \\ \ 2x_n \
            \end{bmatrix} \triangleq 2\mathbf{x}
	    \end{equation*}

        \item
        \textit{Linear transformation.} For $ \forall \mathbf{A} \in
        \mathbb{R}^{m \times n} $, $ \nabla \mathbf{Ax} = \mathbf{A} $.
        \begin{equation*}
            \mathbf{Ax} = \begin{bmatrix}
                \ \sum_{k = 1}^na_{1k}x_k \ \\ \ \vdots \ \\
                \ \sum_{k = 1}^na_{mk}x_k \
            \end{bmatrix} \in \mathbb{R}^m \Rightarrow
            \nabla\mathbf{Ax} = \begin{bmatrix}
                \ a_{11} & \ldots & a_{1n} \ \\ \ \vdots \ \\
                \ a_{m1} & \ldots & a_{mn} \ 
            \end{bmatrix} \triangleq \mathbf{A}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Derivative cookbook}
    \begin{itemize} 
        \item
        \textit{Quadratic form.} $ \forall \mathbf{Q} \in
        \mathbb{R}^{n \times n} $ symmetric,
        \begin{equation*}
            \begin{split}
                \mathbf{x}^\top\mathbf{Qx} & = \sum_{j = 1}^n\sum_{k = 1}^n
	                q_{jk}x_jx_k = \sum_{j = 1}^nq_{jj}x_j^2 +
	                2\sum_{j = 1}^n\sum_{k = j + 1}^nq_{jk}x_jx_k \\      
                & \Rightarrow \nabla \mathbf{x}^\top\mathbf{Qx} =
                \begin{bmatrix}
	                \ \sum_{k = 1}^n2q_{1k}x_k \ \\ \ \vdots \ \\
	                \ \sum_{k = 1}^n2q_{nk}x_k \
	            \end{bmatrix} \triangleq 2\mathbf{Qx} \\
	            & \Rightarrow \nabla^2\mathbf{x}^\top\mathbf{Qx} =
	            \nabla\left(\nabla\mathbf{x}^\top\mathbf{Qx}\right) =
	            \begin{bmatrix}
                    \ 2q_{11} & \ldots & 2q_{1n} \ \\
                    \ \vdots & \ddots & \vdots \ \\
                    \ 2q_{n1} & \ldots & 2q_{nn} \
                \end{bmatrix} \triangleq 2\mathbf{Q}
            \end{split}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Derivative cookbook}
    \begin{itemize}
        \item
        \textit{Multivariate chain rule.} Suppose $ h : \mathbb{R}^n
        \rightarrow \mathbb{R} $ is the \textit{composition} of $ f :
        \mathbb{R}^m \rightarrow \mathbb{R} $, $ g : \mathbb{R}^n
        \rightarrow \mathbb{R}^m $, i.e. $ h \triangleq f \circ g $. Then,
        $ \forall \mathbf{x} \in D \subseteq \mathbb{R}^n $, $ D $ the set of
        points where $ \nabla h : \mathbb{R}^n \rightarrow \mathbb{R}^n $ is
        defined,
        \begin{equation*}
            \nabla h(\mathbf{x}) = [\nabla g(\mathbf{x})]^\top\nabla f(g(\mathbf{x}))
        \end{equation*}
        Here $ \nabla f(g(\mathbf{x})) \in \mathbb{R}^m $,
        $ \nabla g(\mathbf{x}) \in \mathbb{R}^{m \times n} $.
        In the special case that $ m = 1 $, we simply have
        $ \nabla h(\mathbf{x}) = f'(g(\mathbf{x}))\nabla g(\mathbf{x}) $.

        \item
        \textit{Remark.} The fact that the $ n \times 1 $ gradient is a
        transposed $ 1 \times n $ Jacobian can lead to some confusion,
        especially with the overloading of the $ \nabla $ operator. Context
        is important.
    \end{itemize}
\end{frame}

% BibTeX slide for references. should use either acm or ieeetr style
\begin{frame}{References}
    \bibliographystyle{acm}
    \bibliography{../master_bib}
\end{frame}

\end{document}