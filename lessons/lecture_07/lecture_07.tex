%%% BAT lecture 07 %%%
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e, amsmath, amssymb, amsfonts, graphicx}
% allow section.equation numbering
\numberwithin{equation}{section}
% use boadilla theme
\usetheme{Boadilla}
% remove navigation symbols
\usenavigationsymbolstemplate{}
% get numbered figure captions
\setbeamertemplate{caption}[numbered]
% changes itemize to circle + other things
\useoutertheme{split}
\useinnertheme{circles}

% command for the title string. change for each lecture
\newcommand{\lecturetitle}{Intro to Optimization, Part 2}
% allow automatic alert-highlighted references and hyperlinks
\newcommand{\aref}[1]{\alert{\ref{#1}}}
\newcommand{\ahref}[2]{\href{#1}{\alert{#2}}}
% title page stuff. brackets content displayed in footer bar
\title[\lecturetitle]{\lecturetitle}
% metadata. content in brackets is displayed in footer bar
\author[Derek Huang (BAC Advanced Team)]{Derek Huang}
\institute{BAC Advanced Team}
\date{June 2, 2021}

% change "ball" bullet to numbered bullet and section title for section
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
% change ball to gray square (copied from stackoverflow; \par needed for break)
\setbeamertemplate{subsection in toc}{        
    \hspace{1.2em}{\color{gray}\rule[0.3ex]{3pt}{3pt}}~\inserttocsubsection\par
}
% use default enumeration scheme
\setbeamertemplate{enumerate items}[default]
% required line that fixes the problem of \mathbf, \bf not working in beamer
% for later (post-2019) TeX Live installations. see the issue on GitHub:
% https://github.com/josephwright/beamer/issues/630
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{}

\begin{document}

% title slide
\begin{frame}
    \titlepage
    \centering
    % relative path may need to be updated depending on .tex file location
    \includegraphics[scale = 0.1]{../bac_logo1.png}
\end{frame}

% table of contents slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

\section{Unconstrained optimization}

\begin{frame}{Motivation}
    \begin{itemize}
        \item
        Let $ \mathbf{X} \in \mathbb{R}^{N \times d} $ be the centered input
        matrix, $ \mathbf{y} \in \mathbb{R}^N $ the centered response
        vector\footnote{
            As a reminder, centering $ \mathbf{X} $, $ \mathbf{y} $ allows us
            to fit an interceptless model.
        }.
        Suppose we want to fit a lasso model to $ \mathbf{X}, \mathbf{y} $.

        \item
        In Lagrangian form, for $ \lambda \in (0, \infty) $, we must solve
        \begin{equation} \label{eq:lasso_obj}
            \begin{array}{ll}
                \displaystyle\min_\mathbf{w} &
                \Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2 +
                \lambda\Vert\mathbf{w}\Vert_1
            \end{array}
        \end{equation}

        \item
        Theory tells us (\aref{eq:lasso_obj}) is convex but not
        differentiable. No closed form.

        \item
        Iterative methods exist\footnote{
            The scikit-learn \ahref{%
                https://scikit-learn.org/stable/modules/generated/%
                sklearn.linear_model.Lasso.html%
            }{\texttt{Lasso}} model in particular is fit using
            coordinate descent.
        } to solve (\aref{eq:lasso_obj}) and similar
        problems. Software exists, but as users, we want to know \textbf{what}
        to use and \textbf{when}.

        \item
        \alert{
            No algorithm is the ``best'' choice for all problems, so we must
            tailor our choice of algorithm to the problem at hand.
        }
    \end{itemize}
\end{frame}

\subsection{Line search}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Consider the generic unconstrained\footnote{
            Most machine learning problems can be written as unconstrained
            problems.
        } minimization problem
        \begin{equation} \label{eq:unconstrained_min}
            \begin{array}{ll}
                \displaystyle\min_\mathbf{x} & f(\mathbf{x})
            \end{array}
        \end{equation}
        $ f : \mathbb{R}^n \rightarrow \mathbb{R} $ is a differentiable
        objective. Let $ \mathbf{x}_{t - 1} \in \mathbb{R}^n $ denote the
        parameter estimate at the end of iteration $ t - 1 $,
        $ t \in \mathbb{N} $.

        \item
        Many algorithms for computing $ \mathbf{x}_t $ are based on
        \textit{line search}\footnote{
            We do not discuss the \textit{trust region} approach, the other
            broad class of methods.
        }, which computes a \textit{search direction}
        $ \mathbf{d}_t \in \mathbb{R}^n $, \textit{step size}
        $ \eta_t > 0 $ s.t. \cite{nocedal_opt}
        \begin{equation} \label{eq:line_search_eq}
            \mathbf{x}_t = \mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t
        \end{equation}

        \item
        Obvious question is how to choose $ \mathbf{d}_t $, $ \eta_t $. Often
        $ \mathbf{d}_t $ is required to be a \textit{descent direction}, i.e.
        $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0 $, where for small
        enough $ \eta_t $, $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0
        \Rightarrow f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $ \cite{nocedal_opt}.
    \end{itemize}

    % more spacing for the footnote
    \medskip
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        \textit{Proof.} Consider Taylor expansion for $ f(\mathbf{x}_{t - 1} +
        \eta_t\mathbf{d}_t) $ around $ \mathbf{x}_{t - 1} $
        \begin{equation*}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) =
            f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}_t^\top
            \nabla f(\mathbf{x}_{t - 1}) + O(\eta_t^2)
        \end{equation*}
        To leading order, $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0
        \Rightarrow f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $, as long as $ \eta_t $ small enough,
        i.e. $ \eta_t\mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) $ dominates
        $ O(\eta_t^2) $ terms.

        \item
        Geometrically, if $ \mathbf{d}_t $ is a descent direction, then the
        angle $ \theta $ between $ \mathbf{d}_t $,
        $ -\nabla f(\mathbf{x}_{t - 1}) $ is in $ [0, \pi / 2) $, i.e.
        $ \theta $ is an acute angle.

        \item
        In practice, for $ \mathbf{H}_t \in \mathbb{R}^{n \times n} $,
        $ \mathbf{H}_t = \mathbf{H}_t^\top $, $ |\mathbf{H}_t| \ne 0 $, we
        often choose \cite{nocedal_opt}
        \begin{equation} \label{eq:gen_descent_step}
            \mathbf{d}_t = -\mathbf{H}_t^{-1}\nabla f(\mathbf{x}_{t - 1})
        \end{equation}
        Note when $ \mathbf{H}_t \succ \mathbf{0} $, $ \mathbf{d}_t^\top
        \nabla f(\mathbf{x}_{t - 1}) = -\nabla f(\mathbf{x}_{t - 1})^\top
        \mathbf{H}_t^{-1}\nabla f(\mathbf{x}_{t - 1}) < 0 $. In other words,
        $ \mathbf{H}_t \succ \mathbf{0} \Rightarrow \mathbf{d}_t $ is a
        descent direction.
    \end{itemize}
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Assuming $ \mathbf{d}_t $ computed, the optimal choice of $ \eta_t $
        is such that
        \begin{equation} \label{eq:exact_line_search}
            \eta_t = \arg\min_{\eta > 0}
            f(\mathbf{x}_{t - 1} + \eta\mathbf{d}_t)
        \end{equation}
        Too expensive to compute in general, but simply choosing $ \eta_t $
        s.t. $ f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $ does \textbf{not} guarantee convergence
        \cite{nocedal_opt}.

        \item
        Instead, require ``sufficient decrease'' in $ f $ each iteration.
        A common step size rule is the \textit{Armijo rule}, which for
        $ \alpha \in (0, 1) $, requires
        \begin{equation} \label{eq:armijo_rule}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) \le
            f(\mathbf{x}_{t - 1}) + \alpha\eta_t\mathbf{d}_t^\top
            \nabla f(\mathbf{x}_{t - 1})
        \end{equation}
        Note $ \mathbf{d}_t $ is a descent direction $ \Rightarrow
        \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0 $.
        (\aref{eq:armijo_rule}) stipulates $ f(\mathbf{x}_{t - 1} +
        \eta_t\mathbf{d}_t) $ must lie under a damped linear extrapolation.

        \item
        For convex $ f $, $ \{\eta_t\}_{t \in \mathbb{N}} $ satisfying
        (\aref{eq:armijo_rule}) and descent directions
        $ \{\mathbf{d}_t\}_{t \in \mathbb{N}} $ \textbf{guarantee} that the
        global minimizer of $ f $ will be reached \cite{stat_learn_sparsity}.
    \end{itemize}

    % give more space on the bottom for aesthetic
    \medskip
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Although (\aref{eq:armijo_rule}) is not sufficient to ensure
        reasonable progress along $ \mathbf{d}_t $, it is sufficient if
        $ \eta_t $ is chosen using \textit{backtracking} \cite{nocedal_opt}.

        % more space for algorithm
        \medskip

	    \begin{centering}
	    \scalebox{0.9}{
		    \begin{algorithm}[H]
		        % inputs
		        \KwIn{%
		            $ f : \mathbb{R}^n \rightarrow \mathbb{R} $,
		            $ \mathbf{x}_{t - 1}, \mathbf{d}_t \in \mathbb{R}^n $,
		            $ \eta_0 \in (0, \infty) $, $ \alpha, \gamma \in (0, 1) $%
		        }
		        % outputs
		        \KwOut{$ \eta_t \in (0, \eta_0) $}
		        %% pseudocode %%
		        $ \eta_t \leftarrow \eta_0 $ \\
		        \While{%
		            $ f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) >
		            f(\mathbf{x}_{t - 1}) + \alpha\eta_t\mathbf{d}_t^\top
		            \nabla f(\mathbf{x}_{t - 1}) $%
		        }{
		            $ \eta_t \leftarrow \gamma\eta_t $ \\
		        }
		        \KwRet{$ \eta_t $}
		        \label{algo:backtrack_line_search}
		        \caption{Backtracking line search}
		    \end{algorithm}
		}
	    \end{centering}

        % more space for algorithm
	    \medskip

        \item
        Algorithm \aref{algo:backtrack_line_search} guarantees a
        $ \eta_t \in (0, \eta_0) $ that satisfies (\aref{eq:armijo_rule}). 
        Since $ \eta_t $ is decreased from $ \eta_0 $ until $ \eta_t $
        satisfies (\aref{eq:armijo_rule}), $ \eta_t $ is not ``too short''
        \cite{nocedal_opt}.

        \item
        How to choose $ \alpha $, $ \gamma $? For convex $ f $,
        \cite{stat_learn_sparsity} states $ \alpha = 0.5 $, $ \gamma = 0.8 $
        is reasonable, although \cite{bv_convex_opt} recommends
        $ \alpha \in [0.01, 0.3] $, $ \gamma \in [0.1, 0.8] $.
    \end{itemize}
\end{frame}

\subsection{Gradient descent}

\begin{frame}{Gradient descent}
    \begin{itemize}
        \item
        Gradient descent chooses $ \mathbf{d}_t =
        -\nabla f(\mathbf{x}_{t - 1}) $, i.e. $ \mathbf{H}_t = \mathbf{I} $.
        By first-order Taylor approximation around $ \mathbf{x}_{t - 1} $ for
        small enough\footnote{
            Again, in the sense that $ \eta_t\mathbf{d}^\top
            \nabla f(\mathbf{x}_{t - 1}) $ dominates $ O(\eta_t^2) $ terms.
        } $ \eta_t $, we have
        \begin{equation*}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}) \approx
            f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}^\top
            \nabla f(\mathbf{x}_{t - 1}) \triangleq
            \hat{f}_{t; T_1}(\eta_t\mathbf{d})
        \end{equation*}
        The unit vector\footnote{
            If we don't constrain $ \Vert\mathbf{d}\Vert_2 $,
            (\aref{eq:grad_descent_opt}) is unbounded below.
        } $ \hat{\mathbf{d}} $ minimizing $ \hat{f}_{t; T_1} $ solves
        \cite{nocedal_opt}
        \begin{equation} \label{eq:grad_descent_opt}
            \begin{array}{ll}
                \displaystyle\min_{\mathbf{d}} &
                    \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) \\
                \text{s.t.} & \Vert\mathbf{d}\Vert_2 = 1
            \end{array}
        \end{equation}

        \item
        Since $ \forall \mathbf{d} \in \mathbb{R}^n $,
        $ \Vert\mathbf{d}\Vert_2 = 1 $, $ \mathbf{d}^\top
        \nabla f(\mathbf{x}_{t - 1}) = \Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2
        \cos\theta $, solution to (\aref{eq:grad_descent_opt}) results in
        $ \cos\theta = -1 \Leftrightarrow \theta = \pi \Leftrightarrow
        \hat{\mathbf{d}} = -\frac{
            \nabla f(\mathbf{x}_{t - 1})
        }{\Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2} $.

        \item
        $ -\nabla f(\mathbf{x}_{t - 1}) $ is the
        \textit{steepest descent direction} w.r.t to the $ \ell^2 $ norm
        \cite{bv_convex_opt}.
    \end{itemize}

    % spacing for footnotes
    \bigskip
\end{frame}

\begin{frame}{Gradient descent}
    \begin{itemize}
        \item
        Choosing $ \mathbf{d}_t = -\nabla f(\mathbf{x}_{t - 1}) $ is simple
        and computationally cheap.

        \item
        However, gradient descent has a slow \textit{rate of convergence},
        roughly meaning that error reduction per iteration is relatively slow.

        \item
        In general, if $ \exists \nabla^2f $ and gradient descent converges to
        a local minimum $ \mathbf{x}^* $ where $ \nabla^2f(\mathbf{x}^*) \succ
        \mathbf{0} $, $ \forall $ sufficiently large $ t $, we have\footnote{
            (\aref{eq:grad_descent_conv}) assumes
            $ \eta_t = \arg\min_{\eta > 0}f(\mathbf{x}_{t - 1} - \eta
            \nabla f(\mathbf{x}_{t - 1})) $, i.e. we use \textit{exact}
            line search.
        }
        \cite{nocedal_opt}
        \begin{equation} \label{eq:grad_descent_conv}
            f(\mathbf{x}_t) - f(\mathbf{x}^*) \le q^2(f(\mathbf{x}_{t - 1}) -
            f(\mathbf{x}^*))
        \end{equation}
        Here $ q \in \left(\frac{\lambda_{\max} -
        \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}, 1\right) $,
        $ \lambda_{\max} $, $ \lambda_{\min} $ respectively the largest and
        smallest eigenvalues of $ \nabla^2f(\mathbf{x}^*) $. 
        (\aref{eq:grad_descent_conv}) indicates
        $ Q $\textit{-linear convergence}.

        \item
        In other words, for large enough $ t $, the next estimate of
        $ \mathbf{x}^* $ reduces the error by some constant dependent on the
        \textit{condition}\footnote{
            Since $ \nabla^2f(\mathbf{x}^*) \succ \mathbf{0} $, a larger
            \textit{condition number} $ \kappa\big(\nabla^2f(\mathbf{x}^*)\big)
            \Rightarrow \lambda_{\max} / \lambda_{\min} $ is larger.
        } of $ \nabla^2 f(\mathbf{x}^*) $.
    \end{itemize}

    % more space for footnote
    \medskip
\end{frame}

\subsection{Newton's method}

\begin{frame}{Newton's method}
    \begin{itemize}
        \item
        Can we improve rate of convergence given more knowledge about $ f $?

        \item
        Suppose $ \forall \mathbf{x} \in \mathbb{R}^n $,
        $ \nabla^2f(\mathbf{x}) \succ \mathbf{0} $. By second-order Taylor
        expansion around $ \mathbf{x}_{t - 1} $ for small\footnote{
            In the sense that the $ O(\Vert\mathbf{d}\Vert_2^3) $ terms are
            dominated by leading order terms.        
        } $ \mathbf{d} \in \mathbb{R}^n $, $ f(\mathbf{x}_{t - 1} +
        \mathbf{d}) \approx \hat{f}_{t; T_2}(\mathbf{d}) $, where
        \begin{equation*}
            \hat{f}_{f; T_2}(\mathbf{d}) \triangleq f(\mathbf{x}_{t - 1}) +
                \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) +
                \frac{1}{2}\mathbf{d}^\top
                \nabla^2f(\mathbf{x}_{t - 1})\mathbf{d}
        \end{equation*}
        The vector $ \hat{\mathbf{d}} $ minimizing $ \hat{f}_{t; T_2} $ solves
        the unconstrained convex QP
        \begin{equation} \label{eq:newton_descent_opt}
            \begin{array}{ll}
                \displaystyle\min_{\mathbf{d}} &
                    \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) +
                    \frac{1}{2}\mathbf{d}^\top
                    \nabla^2f(\mathbf{x}_{t - 1})\mathbf{d}
            \end{array}
        \end{equation}

        \item
        Newton's method chooses $ \mathbf{d}_t =
        -\nabla^2f(\mathbf{x}_{t - 1})^{-1}\nabla f(\mathbf{x}_{t - 1}) =
        \hat{\mathbf{d}} $, i.e. $ \mathbf{H}_t =
        \nabla^2f(\mathbf{x}_{t - 1}) $. Possible $ \Leftrightarrow
        \exists\nabla^2f $, $ \left|\nabla^2f(\mathbf{x})\right| \ne 0 $,
        $ \forall \mathbf{x} \in \mathbb{R}^n $.


        \item
        From (\aref{eq:gen_descent_step}), the Newton $ \mathbf{d}_t $ is a
        descent direction $ \Leftrightarrow \nabla^2f(\mathbf{x}_{t - 1})
        \succ \mathbf{0} $\footnote{
            If $ \nabla^2 f(\mathbf{x}_{t - 1}) \not\succ \mathbf{0} $,
            sometimes $ \mathbf{d}_t \leftarrow \tilde{\mathbf{H}}_t^{-1}
            \nabla f(\mathbf{x}_{t - 1}) $, $ \tilde{\mathbf{H}}_t \succ
            \mathbf{0} $ based on $ \nabla^2f(\mathbf{x}_{t - 1}) $
            \cite{nocedal_opt}.
        }.
    \end{itemize}

    % again, more spacing for footnote
    \bigskip
\end{frame}

\begin{frame}{Newton's method}
    \begin{itemize}
        \item
        More assumptions on $ f $ are required to describe local convergence.

        \item
        % we can't use footmisc for multiple footnotes because hyperref does
        % not really place nice with footmisc. the quick and acceptably dirty
        % method is to use \textsuperscript{,} between footnotes.
        Let $ \mathbf{x}^* $ be a local minimum s.t. $ \forall \mathbf{x},
        \mathbf{z} \in \{\mathbf{x}' :
        \Vert\mathbf{x}' - \mathbf{x}^*\Vert_2 < r\} $, $ r > 0 $,
        $ \Vert\nabla^2f(\mathbf{x}) - \nabla^2f(\mathbf{z})\Vert_2 \le
        L\Vert\mathbf{x} - \mathbf{z}\Vert_2 $, $ \nabla^2f(\mathbf{x}^*)
        \succ \mathbf{0} $. Then\footnote{
            The \textit{spectral norm} $ \Vert\mathbf{A}\Vert_2 $ of
            $ \mathbf{A} \in \mathbb{R}^{m \times n} $ gives the largest
            singular value of $ \mathbf{A} $.
        }\textsuperscript{,}\footnote{
            (\aref{eq:newton_conv}) requires $ \exists r^* > 0 $ s.t.
            $ \Vert\mathbf{x}_0  - \mathbf{x}^*\Vert_2 \le \min\big\{r^*,
            (2L\Vert\nabla^2f(\mathbf{x}^*)^{-1}\Vert_2)^{-1}\big\} $,
            $ \forall t \in \mathbb{N} $,
            \begin{equation*}
                \Vert\mathbf{x}_{t - 1} - \mathbf{x}^*\Vert_2 \le r^*
                \Rightarrow
                \big\Vert\nabla^2f(\mathbf{x}_{t - 1})^{-1}\big\Vert_2 \le
                2\big\Vert\nabla^2f(\mathbf{x}^*)^{-1}\big\Vert_2
            \end{equation*}
            % remove excess empty space
            \vspace{-10 pt}
        } \cite{nocedal_opt},
        \begin{equation} \label{eq:newton_conv}
            \Vert\mathbf{x}_t - \mathbf{x}^*\Vert_2 \le
            L\left\Vert\nabla^2f(\mathbf{x}^*)^{-1}\right\Vert_2
            \Vert\mathbf{x}_{t - 1} - \mathbf{x}^*\Vert_2^2
        \end{equation}
        (\aref{eq:newton_conv}) indicates $ Q $\textit{-quadratic convergence},
        much faster than $ Q $-linear.

        \item
        $ \forall $ large enough $ t $, $ \eta_t = 1 $, the ``natural'' step
        size, satisfies (\aref{eq:armijo_rule}) \cite{nocedal_opt}.

        \item
        Note that even if $ \exists \nabla^2 f $, $ \nabla^2f(\mathbf{x}) $ is
        expensive to compute and store for large $ n $. In supervised learning,
        often $ \nabla^2 f $ unknown and/or $ n $ large.
    \end{itemize}

    % make non-footnote text aligned better
    \medskip
\end{frame}

\section{Extending gradient descent}

\subsection{Proximal gradient descent}

%\begin{frame}{Proximal gradient descent}
%    \begin{itemize}
%        \item
%        Can naturally extend gradient descent given a convex feasible set
%        $ \mathcal{C} \subseteq \mathbb{R}^n $, resulting in
%        \textit{projected gradient descent}, where given $ \eta_t $
%        \cite{stat_learn_sparsity},
%        \begin{equation} \label{eq:proj_grad_update}
%            \mathbf{x}_t = \mathcal{P}_\mathcal{C}(\mathbf{x}_{t - 1} -
%            \eta_t\nabla f(\mathbf{x}_{t - 1}))
%        \end{equation}
%        $ \mathcal{P}_\mathcal{C}(\mathbf{x}) \triangleq
%        \arg\min_{\mathbf{z} \in \mathcal{C}}\Vert\mathbf{z} -
%        \mathbf{x}\Vert_2^2 $ gives Euclidean projection of $ \mathbf{x} $
%        onto $ \mathcal{C} $. To require $ \mathbf{x}_t \in \mathcal{C} $, we
%        project a normal gradient step back onto $ \mathcal{C} $.
%    \end{itemize}
%\end{frame}

\begin{frame}{Proximal gradient descent}
    \begin{itemize}
        \item
        Some important supervised learning problems have $ f = g + h $, where
        $ \exists\nabla g $, $ \not\exists\nabla h $, with $ h $ convex, e.g.
        $ \ell^1 $-constrained problems.

        \item
        Since $ \not\exists\nabla f $, we cannot use methods that require
        $ \nabla f $. However, we can use \textit{proximal gradient descent},
        where given $ \mathbf{x}_{t - 1} $, $ \eta_t $, we have
        \cite{stat_learn_sparsity}
        \begin{equation} \label{eq:prox_grad_update}
            \mathbf{x}_t = \operatorname{prox}_{\eta_t h}(\mathbf{x}_{t - 1} -
            \eta_t\nabla g(\mathbf{x}_{t - 1}))
        \end{equation}
        The \textit{proximal operator} $ \operatorname{prox}_{\eta h} $ of
        convex $ h $, $ \eta \in (0, \infty) $, is s.t.
        \cite{stat_learn_sparsity}
        \begin{equation} \label{eq:prox_operator}
            \operatorname{prox}_{\eta h}(\mathbf{x}) \triangleq
            \arg\min_\mathbf{z}\left\{
                \frac{1}{2\eta}\Vert\mathbf{z} - \mathbf{x}\Vert_2^2 +
                h(\mathbf{z})
            \right\}
        \end{equation}

        \item
        One concrete\footnote{
            \cite{prox_algos} provides additional interpretations of
            $ \operatorname{prox}_{\eta h} $ and proximal gradient descent.
        } interpretation of proximal gradient descent is as a case of
        \textit{majorization-minimization}, as described in \cite{prox_algos}.
    \end{itemize}

    % more space for footnote
    \medskip
\end{frame}

\begin{frame}{Proximal gradient descent}
    \begin{itemize}
        \item
        Given $ \mathbf{x}_{t - 1} $, majorization-minimization chooses
        $ \mathbf{x}_t $ s.t. \cite{prox_algos}
        \begin{equation} \label{eq:mm_update}
            \mathbf{x}_t = \arg\min_\mathbf{x}\hat{f}(\mathbf{x},
            \mathbf{x}_{t - 1})
        \end{equation}
        $ \hat{f} $ is called a \textit{majorizing function} for $ f $, or
        said to \textit{majorize} $ f $, where $ \forall \mathbf{x},
        \mathbf{z} \in \mathbb{R}^n $, $ \hat{f}(\cdot, \mathbf{z}) $ is
        convex and $ \hat{f}(\mathbf{x}, \mathbf{z}) \ge f(\mathbf{x}) $,
        $ \hat{f}(\mathbf{x}, \mathbf{x}) = f(\mathbf{x}) $ \cite{prox_algos}.

        \item
        Majorization-minimization ``works'' since $ \forall t \in \mathbb{N} $,
        (\aref{eq:mm_update}) guarantees that $ f(\mathbf{x}_t) \le
        \hat{f}(\mathbf{x}_t, \mathbf{x}_{t - 1}) \le
        \hat{f}(\mathbf{x}_{t - 1}, \mathbf{x}_{t - 1}) =
        f(\mathbf{x}_{t - 1}) $ \cite{stat_learn_sparsity}.

        \item
        Suppose $ \nabla g $ is $ L $-Lipschitz\footnote{
            $ \nabla g $ $ L $-Lipschitz $ \Rightarrow \exists L > 0 $ s.t.
            $ \forall \mathbf{x}, \mathbf{z} \in \mathbb{R}^n $,
            $ \Vert\nabla g(\mathbf{x}) - \nabla g(\mathbf{z})\Vert_2 \le
            L\Vert\mathbf{x} - \mathbf{z}\Vert_2 $.
        }. Define $ \hat{f}_\eta : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} $, where
        \begin{equation} \label{eq:prox_grad_mm_func}
            \hat{f}_\eta(\mathbf{x}, \mathbf{z}) \triangleq f(\mathbf{z}) +
            (\mathbf{x} - \mathbf{z})^\top\nabla g(\mathbf{z}) +
            \frac{1}{2\eta}\Vert\mathbf{x} - \mathbf{z}\Vert_2^2 +
            h(\mathbf{x})
        \end{equation}
        For $ \eta \in (0, 1 / L] $, $ \forall \mathbf{x} \in \mathbb{R}^n $,
        $ \hat{f}_\eta $ is a majorizing function for $ f $ \cite{prox_algos}.
    \end{itemize}

    % more space for footnote
    \medskip
\end{frame}

\begin{frame}{Proximal gradient descent}
    \begin{itemize}
        \item
        Using $ \hat{f}_{\eta_t} $ from (\aref{eq:prox_grad_mm_func}),
        $ \eta_t \in (0, 1 / L] $, the majorization-minimization update is
        identical\footnote{
            For $ \eta_t > 1 / L $, $ \hat{f}_{\eta_t} $ no longer majorizes
            $ f $, but the equivalence still holds.
        } to the proximal gradient update in (\aref{eq:prox_grad_update})
        \cite{prox_algos}, i.e.
        \begin{equation} \label{eq:prox_mm_update_equiv}
            \mathbf{x}_t = \arg\min_\mathbf{x}
            \hat{f}_{\eta_t}(\mathbf{x}, \mathbf{x}_{t - 1}) =
            \operatorname{prox}_{\eta_t h}(\mathbf{x}_{t - 1} -
            \eta_t\nabla g(\mathbf{x}_{t - 1}))
        \end{equation}
        When $ h = 0 $, i.e. $ f = g $, (\aref{eq:prox_mm_update_equiv}) is
        the gradient descent update for $ g $.

        \item
        In particular, when $ h = \lambda\Vert\cdot\Vert_1 $,
        $ \lambda > 0 $, (\aref{eq:prox_grad_update}) reduces to
        \begin{equation} \label{eq:prox_grad_update_l1}
            \mathbf{x}_t = \mathcal{S}_{\eta_t\lambda}(\mathbf{x}_{t - 1} -
            \eta_t\nabla g(\mathbf{x}_{t - 1}))
        \end{equation}
        $ \mathcal{S}_\eta $ is the \textit{soft-thresholding operator}, with
        $ j $th component \cite{stat_learn_sparsity}
        \begin{equation} \label{eq:soft_threshold}
            [\mathcal{S}_\eta(\mathbf{x})]_j =
            \operatorname{sgn}(x_j)\max\{0, |x_j| - \eta\}        
        \end{equation}

        \item
        $ \mathcal{S}_\eta $ is quite cheap\footnote{
            For most $ h $ of interest in statistics,
            $ \operatorname{prox}_{\eta h} $ is not too computationally
            expensive \cite{stat_learn_sparsity}.
        } to compute, making proximal gradient descent a useful method for
        minimization with an $ \ell^1 $ norm penalty.
    \end{itemize}

    % spacing for footnotes
    \bigskip
\end{frame}


\subsection{Nesterov's acceleration}

\begin{frame}{Nesterov's acceleration}
    \begin{itemize}
        \item
        Suppose $ \mathbf{x}_0 $, $ \{\mathbf{x}_t\}_{t \in \mathbb{N}} $ lie
        within convex $ \mathcal{C} \subset \mathbb{R}^n $, $ g|_\mathcal{C} $,
        the restriction of $ g $ to $ \mathcal{C} $, is convex, $ \nabla g $
        is $ L $-Lipschitz, and $ \mathbf{x}^* \in \mathcal{C} $ minimizes
        $ g|_\mathcal{C} $\footnote{
            Result originally stated in \cite{stat_learn_sparsity} for convex
            $ g $ with $ L $-Lipschitz $ \nabla g $.
        }.

        \item
        For $ \{\eta_t\}_{t \in \mathbb{N}} $ chosen by Armijo rule
        (\aref{eq:armijo_rule}) or fixed to $ \tilde{\eta} \in (0, 1 / L] $,
        $ \exists C_1 > 0 $ s.t. under proximal gradient descent, we have
        \cite{stat_learn_sparsity}
        \begin{equation} \label{eq:prox_grad_conv}
            f(\mathbf{x}_t) - f(\mathbf{x}^*) \le \frac{C_1}{t + 1}
            \Vert\mathbf{x}_t - \mathbf{x}^*\Vert_2
        \end{equation}
        In other words, estimation error $ f(\mathbf{x}_t) - f(\mathbf{x}^*) $
        is $ O(1 / t) $.

        \item
        However, we can modify (\aref{eq:prox_grad_update}) to incorporate
        Nesterov's acceleration, i.e. for $ \tilde{\mathbf{x}}_0 =
        \mathbf{x}_0 $, $ \{\tilde{\mathbf{x}}_t\}_{t \in \mathbb{N}} $, the
        estimates $ \{\mathbf{x}_t\}_{t \in \mathbb{N}} $ are s.t.
        \cite{stat_learn_sparsity}
        \begin{subequations} \label{eq:prox_grad_update_accel}
            \begin{align}
            \mathbf{x}_t & = \operatorname{prox}_{\eta_t h}(
                \tilde{\mathbf{x}}_{t - 1} - \eta_t\nabla g(
                    \tilde{\mathbf{x}}_{t - 1}
                )
            ) \label{eq:prox_grad_update_accel_a} \\
            \tilde{\mathbf{x}}_t & = \mathbf{x}_t + \frac{t - 1}{t + 2}(
                \mathbf{x}_t - \mathbf{x}_{t - 1}
            ) \label{eq:prox_grad_update_accel_b}
            \end{align}
        \end{subequations}
    \end{itemize}

    % adjust spacing for footnote
    \medskip
\end{frame}

\begin{frame}{Nesterov's acceleration}
    \begin{itemize}
        \item
        Using the accelerated update in (\aref{eq:prox_grad_update_accel}),
        then $ \exists C_2 > 0 $ s.t. \cite{stat_learn_sparsity}
        \begin{equation} \label{eq:prox_grad_accel_conv}
            f(\mathbf{x}_t) - f(\mathbf{x}^*) \le \frac{C_2}{(t + 1)^2}
            \Vert\mathbf{x}_0 - \mathbf{x}^*\Vert_2
        \end{equation}
        The accelerated updates result in $ f(\mathbf{x}_t) - f(\mathbf{x}^*) $
        being $ O(1 / t^2) $, a significant improvement in estimation error
        over (\aref{eq:prox_grad_conv}).

        \item
        Nesterov's acceleration reduces zig-zagging of
        $ \{\mathbf{x}_t\}_{t \in \mathbb{N}} $ on certain objectives
        \cite{stat_learn_sparsity}, e.g. convex $ g $ or $ g|_\mathcal{C} $
        with anisotropic sublevel sets\footnote{
            Roughly speaking, convex $ g $ or $ g|_\mathcal{C} $ that have
            highly elongated contours.
        } \cite{bv_convex_opt}.

        \item
        Given $ \mathbf{x}_{t - 1} $, can interpret as first overshooting
        along $ \mathbf{x}_{t - 1} - \mathbf{x}_{t - 2} $ (extrapolating)
        before a correcting proximal gradient step.

        \item
        The gradient step opposes the overshoot, resulting in
        $ \{\mathbf{x}_t\}_{t \in \mathbb{N}} $ zig-zagging less and
        progressing more efficiently towards $ \mathbf{x}^* $.
    \end{itemize}

    % more spacing for footnote
    \medskip
\end{frame}

% BibTeX slide for references. should use either acm or ieeetr style
\begin{frame}{References}
    \bibliographystyle{acm}
    % relative path may need to be updated depending on .tex file location
    \bibliography{../master_bib}
\end{frame}

\end{document}