%%% BAT lecture 07 %%%
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e, amsmath, amssymb, amsfonts, graphicx}
% allow section.equation numbering
\numberwithin{equation}{section}
% use boadilla theme
\usetheme{Boadilla}
% remove navigation symbols
\usenavigationsymbolstemplate{}
% get numbered figure captions
\setbeamertemplate{caption}[numbered]
% changes itemize to circle + other things
\useoutertheme{split}
\useinnertheme{circles}

% command for the title string. change for each lecture
\newcommand{\lecturetitle}{Intro to Optimization, Part 2}
% allow automatic alert-highlighted references and hyperlinks
\newcommand{\aref}[1]{\alert{\ref{#1}}}
\newcommand{\ahref}[2]{\href{#1}{\alert{#2}}}
% title page stuff. brackets content displayed in footer bar
\title[\lecturetitle]{\lecturetitle}
% metadata. content in brackets is displayed in footer bar
\author[Derek Huang (BAC Advanced Team)]{Derek Huang}
\institute{BAC Advanced Team}
\date{May 30, 2021}

% change "ball" bullet to numbered bullet and section title for section
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
% change ball to gray square (copied from stackoverflow; \par needed for break)
\setbeamertemplate{subsection in toc}{        
    \hspace{1.2em}{\color{gray}\rule[0.3ex]{3pt}{3pt}}~\inserttocsubsection\par
}
% use default enumeration scheme
\setbeamertemplate{enumerate items}[default]
% required line that fixes the problem of \mathbf, \bf not working in beamer
% for later (post-2019) TeX Live installations. see the issue on GitHub:
% https://github.com/josephwright/beamer/issues/630
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{}

\begin{document}

% title slide
\begin{frame}
    \titlepage
    \centering
    % relative path may need to be updated depending on .tex file location
    \includegraphics[scale = 0.1]{../bac_logo1.png}
\end{frame}

% table of contents slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

\section{Unconstrained optimization}

\begin{frame}{Motivation}
    \begin{itemize}
        \item
        Let $ \mathbf{X} \in \mathbb{R}^{N \times d} $ be the centered input
        matrix, $ \mathbf{y} \in \mathbb{R}^N $ the centered response
        vector\footnote{
            As a reminder, centering $ \mathbf{X} $, $ \mathbf{y} $ allows us
            to fit an interceptless model.
        }.
        Suppose we want to fit a lasso model to $ \mathbf{X}, \mathbf{y} $.

        \item
        In Lagrangian form, for $ \lambda \in (0, \infty) $, we must solve
        \begin{equation} \label{eq:lasso_obj}
            \begin{array}{ll}
                \displaystyle\min_\mathbf{w} &
                \Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2 +
                \lambda\Vert\mathbf{w}\Vert_1
            \end{array}
        \end{equation}

        \item
        Theory tells us (\aref{eq:lasso_obj}) is convex but not
        differentiable. No closed form.

        \item
        Iterative methods exist\footnote{
            Coordinate descent and proximal gradient descent are often used.
        } to solve (\aref{eq:lasso_obj}) and similar
        problems. Software exists, but as users, we want to know \textbf{what}
        to use and \textbf{when}.

        \item
        \alert{
            No algorithm is the ``best'' choice for all problems, so we must
            tailor our choice of algorithm to the problem at hand.
        }
    \end{itemize}
\end{frame}

\subsection{Line search}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Consider the generic unconstrained\footnote{
            Most machine learning problems can be written as unconstrained
            problems.
        } minimization problem
        \begin{equation} \label{eq:unconstrained_min}
            \begin{array}{ll}
                \displaystyle\min_\mathbf{x} & f(\mathbf{x})
            \end{array}
        \end{equation}
        $ f : \mathbb{R}^n \rightarrow \mathbb{R} $ is a differentiable
        objective. Let $ \mathbf{x}_{t - 1} \in \mathbb{R}^n $ denote the
        parameter estimate at the end of iteration $ t - 1 $,
        $ t \in \mathbb{N} $.

        \item
        Many algorithms for computing $ \mathbf{x}_t $ are based on
        \textit{line search}\footnote{
            We do not discuss the \textit{trust region} approach, the other
            broad class of methods.
        }, which computes a \textit{search direction}
        $ \mathbf{d}_t \in \mathbb{R}^n $, \textit{step size}
        $ \eta_t \in (0, \infty) $ s.t. \cite{nocedal_opt}
        \begin{equation} \label{eq:line_search_eq}
            \mathbf{x}_t = \mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t
        \end{equation}

        \item
        Obvious question is how to choose $ \mathbf{d}_t $, $ \eta_t $. Often
        $ \mathbf{d}_t $ is required to be a \textit{descent direction}, i.e.
        $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0 $, where for small
        enough $ \eta_t $, $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0
        \Rightarrow f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $ \cite{nocedal_opt}.
    \end{itemize}

    % more spacing for the footnote
    \medskip
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        \textit{Proof.} Consider Taylor expansion for $ f(\mathbf{x}_{t - 1} +
        \eta_t\mathbf{d}_t) $ around $ \mathbf{x}_{t - 1} $
        \begin{equation*}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) =
            f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}_t^\top
            \nabla f(\mathbf{x}_{t - 1}) + O(\eta_t^2)
        \end{equation*}
        To leading order, $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0
        \Rightarrow f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $, as long as $ \eta_t $ small enough,
        i.e. $ \eta_t\mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) $ dominates
        $ O(\eta_t^2) $ terms.

        \item
        Geometrically, if $ \mathbf{d}_t $ is a descent direction, then the
        angle $ \theta $ between $ \mathbf{d}_t $,
        $ -\nabla f(\mathbf{x}_{t - 1}) $ is in $ [0, \pi / 2) $, i.e.
        $ \theta $ is an acute angle.

        \item
        In practice, for $ \mathbf{H}_t \in \mathbb{R}^{n \times n} $,
        $ \mathbf{H}_t = \mathbf{H}_t^\top $, $ |\mathbf{H}_t| \ne 0 $, we
        often choose \cite{nocedal_opt}
        \begin{equation} \label{eq:gen_descent_step}
            \mathbf{d}_t = -\mathbf{H}_t^{-1}\nabla f(\mathbf{x}_{t - 1})
        \end{equation}
        Note when $ \mathbf{H}_t \succ \mathbf{0} $, $ \mathbf{d}_t^\top
        \nabla f(\mathbf{x}_{t - 1}) = -\nabla f(\mathbf{x}_{t - 1})^\top
        \mathbf{H}_t^{-1}\nabla f(\mathbf{x}_{t - 1}) < 0 $. In other words,
        $ \mathbf{H}_t \succ \mathbf{0} \Rightarrow \mathbf{d}_t $ is a
        descent direction.
    \end{itemize}
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Assuming $ \mathbf{d}_t $ computed, the optimal choice of $ \eta_t $
        is such that
        \begin{equation} \label{eq:exact_line_search}
            \eta_t = \arg\min_{\eta > 0}
            f(\mathbf{x}_{t - 1} + \eta\mathbf{d}_t)
        \end{equation}
        Too expensive to compute in general, but simply choosing $ \eta_t $
        s.t. $ f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $ does \textbf{not} guarantee convergence
        \cite{nocedal_opt}.

        \item
        Instead, require ``sufficient decrease'' in $ f $ each iteration.
        A common condition is the \textit{Armijo condition}, which for
        $ \alpha \in (0, 1) $, requires
        \begin{equation} \label{eq:armijo_rule}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) \le
            f(\mathbf{x}_{t - 1}) + \alpha\eta_t\mathbf{d}_t^\top
            \nabla f(\mathbf{x}_{t - 1})
        \end{equation}
        Note $ \mathbf{d}_t $ is a descent direction $ \Rightarrow
        \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0 $.
        (\aref{eq:armijo_rule}) stipulates $ f(\mathbf{x}_{t - 1} +
        \eta_t\mathbf{d}_t) $ must be under a damped linear extrapolation.

        \item
        For convex $ f $, $ \{\eta_t\}_{t \in \mathbb{N}} $ satisfying
        (\aref{eq:armijo_rule}) and descent directions
        $ \{\mathbf{d}_t\}_{t \in \mathbb{N}} $ \textbf{guarantee} that the
        global minimizer of $ f $ will be reached \cite{stat_learn_sparsity}.
    \end{itemize}
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Although (\aref{eq:armijo_rule}) is not sufficient to ensure
        reasonable progress along $ \mathbf{d}_t $, it is sufficient if
        $ \eta_t $ is chosen using \textit{backtracking} \cite{nocedal_opt}.

        % more space for algorithm
        \medskip

	    \begin{centering}
	    \scalebox{0.9}{
		    \begin{algorithm}[H]
		        % inputs
		        \KwIn{%
		            $ f : \mathbb{R}^n \rightarrow \mathbb{R} $,
		            $ \mathbf{x}_{t - 1}, \mathbf{d}_t \in \mathbb{R}^n $,
		            $ \eta_0 \in (0, \infty) $, $ \alpha, \gamma \in (0, 1) $%
		        }
		        % outputs
		        \KwOut{$ \eta_t \in (0, \eta_0) $}
		        %% pseudocode %%
		        $ \eta_t \leftarrow \eta_0 $ \\
		        \While{%
		            $ f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) >
		            f(\mathbf{x}_{t - 1}) + \alpha\eta_t\mathbf{d}_t^\top
		            \nabla f(\mathbf{x}_{t - 1}) $%
		        }{
		            $ \eta_t \leftarrow \gamma\eta_t $ \\
		        }
		        \KwRet{$ \eta_t $}
		        \label{algo:backtrack_line_search}
		        \caption{Backtracking line search}
		    \end{algorithm}
		}
	    \end{centering}

        % more space for algorithm
	    \medskip

        \item
        Algorithm \aref{algo:backtrack_line_search} guarantees a
        $ \eta_t \in (0, \eta_0) $ that satisfies (\aref{eq:armijo_rule}). 
        Since $ \eta_t $ is decreased from $ \eta_0 $ until $ \eta_t $
        satisfies (\aref{eq:armijo_rule}), $ \eta_t $ is not ``too short''
        \cite{nocedal_opt}.

        \item
        How to choose $ \alpha $, $ \gamma $? For convex $ f $,
        \cite{stat_learn_sparsity} states $ \alpha = 0.5 $, $ \gamma = 0.8 $
        is reasonable, although \cite{bv_convex_opt} recommends
        $ \alpha \in [0.01, 0.3] $, $ \gamma \in [0.1, 0.8] $.
    \end{itemize}
\end{frame}

\subsection{Gradient descent}

\begin{frame}{Gradient descent}
    \begin{itemize}
        \item
        Gradient descent chooses $ \mathbf{d}_t =
        -\nabla f(\mathbf{x}_{t - 1}) $, i.e. $ \mathbf{H}_t = \mathbf{I} $.
        By first-order Taylor approximation around $ \mathbf{x}_{t - 1} $ for
        small enough\footnote{
            Again, in the sense that $ \eta_t\mathbf{d}^\top
            \nabla f(\mathbf{x}_{t - 1}) $ dominates $ O(\eta_t^2) $ terms.
        } $ \eta_t $, we have
        \begin{equation*}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}) \approx
            f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}^\top
            \nabla f(\mathbf{x}_{t - 1}) \triangleq
            \hat{f}_{t; T_1}(\eta_t\mathbf{d})
        \end{equation*}
        The unit vector\footnote{
            If we don't constrain $ \Vert\mathbf{d}\Vert_2 $,
            (\aref{eq:grad_descent_opt}) is unbounded below.
        } $ \hat{\mathbf{d}} $ minimizing $ \hat{f}_{t; T_1} $ solves
        \cite{nocedal_opt}
        \begin{equation} \label{eq:grad_descent_opt}
            \begin{array}{ll}
                \displaystyle\min_{\mathbf{d}} &
                    \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) \\
                \text{s.t.} & \Vert\mathbf{d}\Vert_2 = 1
            \end{array}
        \end{equation}

        \item
        Since $ \forall \mathbf{d} \in \mathbb{R}^n $,
        $ \Vert\mathbf{d}\Vert_2 = 1 $, $ \mathbf{d}^\top
        \nabla f(\mathbf{x}_{t - 1}) = \Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2
        \cos\theta $, solution to (\aref{eq:grad_descent_opt}) results in
        $ \cos\theta = -1 \Leftrightarrow \theta = \pi \Leftrightarrow
        \hat{\mathbf{d}} = -\frac{
            \nabla f(\mathbf{x}_{t - 1})
        }{\Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2} $.

        \item
        $ -\nabla f(\mathbf{x}_{t - 1}) $ is the
        \textit{steepest descent direction} w.r.t to the $ \ell^2 $ norm
        \cite{bv_convex_opt}.
    \end{itemize}

    % spacing for footnotes
    \bigskip
\end{frame}

\begin{frame}{Gradient descent}
    \begin{itemize}
        \item
        Choosing $ \mathbf{d}_t = -\nabla f(\mathbf{x}_{t - 1}) $ leads to the
        simplest gradient-based descent algorithm with the lowest computational
        cost.

        \item
        However, gradient descent has a slow \textit{rate of convergence},
        roughly meaning that error reduction per iteration is relatively slow.

        \item
        In general, if $ \exists \nabla^2f $ and gradient descent converges to
        a local minimum $ \mathbf{x}^* $ where $ \nabla^2f(\mathbf{x}^*) \succ
        \mathbf{0} $, $ \forall $ sufficiently large $ t $, we have
        \cite{nocedal_opt}
        \begin{equation} \label{eq:grad_descent_conv}
            f(\mathbf{x}_t) - f(\mathbf{x}^*) \le q^2(f(\mathbf{x}_{t - 1}) -
            f(\mathbf{x}^*))
        \end{equation}
        Here $ q \in \left(\frac{\lambda_{\max} -
        \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}, 1\right) $,
        $ \lambda_{\max} $, $ \lambda_{\min} $ respectively the largest and
        smallest eigenvalues of $ \nabla^2f(\mathbf{x}^*) $. 
        (\aref{eq:grad_descent_conv}) indicates
        $ Q $\textit{-linear convergence}.

        \item
        In other words, for large enough $ t $, the next estimate of
        $ \mathbf{x}^* $ reduces the error by some constant dependent on the
        \textit{condition}\footnote{
            Since $ \nabla^2f(\mathbf{x}^*) \succ \mathbf{0} $, a larger
            \textit{condition number} $ \kappa\big(\nabla^2f(\mathbf{x}^*)\big)
            \Rightarrow \lambda_{\max} / \lambda_{\min} $ is larger.
        } of $ \nabla^2 f(\mathbf{x}^*) $.
    \end{itemize}

    % more space for footnote
    \medskip
\end{frame}

\subsection{Newton's method}

\begin{frame}{Newton's method}
    \begin{itemize}
        \item
        Can we improve rate of convergence given more knowledge about $ f $?

        \item
        Suppose $ \forall \mathbf{x} \in \mathbb{R}^n $,
        $ \nabla^2f(\mathbf{x}) \succ \mathbf{0} $. By second-order Taylor
        expansion around $ \mathbf{x}_{t - 1} $ for small\footnote{
            In the sense that the $ O(\eta_t^3) $ terms are dominated by
            leading order terms.        
        } $ \eta_t $, $ f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d})
        \approx \hat{f}_{t; T_2}(\eta_t\mathbf{d}) $, where
        \begin{equation*}
            \hat{f}_{f; T_2}(\eta_t\mathbf{d}) \triangleq
                f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}_t^\top
                \nabla f(\mathbf{x}_{t - 1}) + \frac{1}{2}\eta_t^2
                \mathbf{d}_t^\top\nabla^2f(\mathbf{x}_{t - 1})\mathbf{d}_t
        \end{equation*}
        The vector $ \hat{\mathbf{d}} $ minimizing $ \hat{f}_{t; T_2} $ solves
        the unconstrained convex QP
        \begin{equation} \label{eq:newton_descent_opt}
            \begin{array}{ll}
                \displaystyle\min_{\mathbf{d}} &
                    \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) +
                    \frac{1}{2}\mathbf{d}^\top
                    \nabla^2f(\mathbf{x}_{t - 1})\mathbf{d}
            \end{array}
        \end{equation}

        \item
        Newton's method chooses $ \mathbf{d}_t =
        -\nabla^2f(\mathbf{x}_{t - 1})^{-1}\nabla f(\mathbf{x}_{t - 1}) =
        \hat{\mathbf{d}} $, i.e. $ \mathbf{H}_t =
        \nabla^2f(\mathbf{x}_{t - 1}) $. Possible $ \Leftrightarrow
        \exists\nabla^2f $, $ \left|\nabla^2f(\mathbf{x})\right| \ne 0 $,
        $ \forall \mathbf{x} \in \mathbb{R}^n $.

        \item
        From (\aref{eq:gen_descent_step}), the Newton $ \mathbf{d}_t $ is a
        descent direction $ \Leftrightarrow \nabla^2f(\mathbf{x}_{t - 1})
        \succ \mathbf{0} $.
    \end{itemize}

    % again, more spacing for footnote
    \medskip
\end{frame}

%\begin{frame}{Newton's method}
%    \begin{itemize}
%        \item
%        
%    \end{itemize}
%\end{frame}

% BibTeX slide for references. should use either acm or ieeetr style
\begin{frame}{References}
    \bibliographystyle{acm}
    % relative path may need to be updated depending on .tex file location
    \bibliography{../master_bib}
\end{frame}

\end{document}