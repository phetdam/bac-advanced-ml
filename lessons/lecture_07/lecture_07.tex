%%% BAT lecture 07 %%%
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e, amsmath, amssymb, amsfonts, graphicx}
% allow section.equation numbering
\numberwithin{equation}{section}
% use boadilla theme
\usetheme{Boadilla}
% remove navigation symbols
\usenavigationsymbolstemplate{}
% get numbered figure captions
\setbeamertemplate{caption}[numbered]
% changes itemize to circle + other things
\useoutertheme{split}
\useinnertheme{circles}

% command for the title string. change for each lecture
\newcommand{\lecturetitle}{Intro to Optimization, Part 2}
% allow automatic alert-highlighted references and hyperlinks
\newcommand{\aref}[1]{\alert{\ref{#1}}}
\newcommand{\ahref}[2]{\href{#1}{\alert{#2}}}
% title page stuff. brackets content displayed in footer bar
\title[\lecturetitle]{\lecturetitle}
% metadata. content in brackets is displayed in footer bar
\author[Derek Huang (BAC Advanced Team)]{Derek Huang}
\institute{BAC Advanced Team}
\date{May 29, 2021}

% change "ball" bullet to numbered bullet and section title for section
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
% change ball to gray square (copied from stackoverflow; \par needed for break)
\setbeamertemplate{subsection in toc}{        
    \hspace{1.2em}{\color{gray}\rule[0.3ex]{3pt}{3pt}}~\inserttocsubsection\par
}
% use default enumeration scheme
\setbeamertemplate{enumerate items}[default]
% required line that fixes the problem of \mathbf, \bf not working in beamer
% for later (post-2019) TeX Live installations. see the issue on GitHub:
% https://github.com/josephwright/beamer/issues/630
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{}

\begin{document}

% title slide
\begin{frame}
    \titlepage
    \centering
    % relative path may need to be updated depending on .tex file location
    \includegraphics[scale = 0.1]{../bac_logo1.png}
\end{frame}

% table of contents slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

\section{Unconstrained optimization}

\begin{frame}{Motivation}
    \begin{itemize}
        \item
        Let $ \mathbf{X} \in \mathbb{R}^{N \times d} $ be the centered input
        matrix, $ \mathbf{y} \in \mathbb{R}^N $ the centered response
        vector\footnote{
            As a reminder, centering $ \mathbf{X} $, $ \mathbf{y} $ allows us
            to fit an interceptless model.
        }.
        Suppose we want to fit a lasso model to $ \mathbf{X}, \mathbf{y} $.

        \item
        In Lagrangian form, for $ \lambda \in (0, \infty) $, we must solve
        \begin{equation} \label{eq:lasso_obj}
            \begin{array}{ll}
                \displaystyle\min_\mathbf{w} &
                \Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2 +
                \lambda\Vert\mathbf{w}\Vert_1
            \end{array}
        \end{equation}

        \item
        Theory tells us (\aref{eq:lasso_obj}) is convex but not
        differentiable. No closed form.

        \item
        Iterative methods exist\footnote{
            Coordinate descent and proximal gradient descent are often used.
        } to solve (\aref{eq:lasso_obj}) and similar
        problems. Software exists, but as users, we want to know \textbf{what}
        to use and \textbf{when}.

        \item
        \alert{We tailor our choice of algorithm to the problem at hand.}
    \end{itemize}
\end{frame}

\subsection{Line search}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Consider the generic unconstrained\footnote{
            Most machine learning problems can be written as unconstrained
            problems.
        } minimization problem
        \begin{equation} \label{eq:unconstrained_min}
            \begin{array}{ll}
                \displaystyle\min_\mathbf{x} & f(\mathbf{x})
            \end{array}
        \end{equation}
        $ f : \mathbb{R}^n \rightarrow \mathbb{R} $ is a differentiable
        objective. Let $ \mathbf{x}_{t - 1} \in \mathbb{R}^n $ denote the
        parameter estimate at the end of iteration $ t - 1 $,
        $ t \in \mathbb{N} $.

        \item
        Many algorithms for computing $ \mathbf{x}_t $ are based on
        \textit{line search}, which computes a \textit{search direction}
        $ \mathbf{d}_t \in \mathbb{R}^n $, \textit{step size}
        $ \eta_t \in (0, \infty) $ s.t. \cite{nocedal_opt}
        \begin{equation} \label{eq:line_search_eq}
            \mathbf{x}_t = \mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t
        \end{equation}

        \item
        Obvious question is how to choose $ \mathbf{d}_t $, $ \eta_t $. Often
        $ \mathbf{d}_t $ is required to be a \textit{descent direction}, i.e.
        $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0 $, where for small
        enough $ \eta_t $, $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0
        \Rightarrow f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $ \cite{nocedal_opt}.
    \end{itemize}

    % more spacing for the footnote
    \medskip
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        \textit{Proof.} Consider Taylor expansion for $ f(\mathbf{x}_{t - 1} +
        \eta_t\mathbf{d}_t) $ around $ \mathbf{x}_{t - 1} $
        \begin{equation*}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) =
            f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}_t^\top
            \nabla f(\mathbf{x}_{t - 1}) + O(\eta_t^2)
        \end{equation*}
        To leading order, $ \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0
        \Rightarrow f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $, as long as $ \eta_t $ small enough,
        i.e. $ \eta_t\mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) $ dominates
        $ O(\eta_t^2) $ terms.

        \item
        Geometrically, if $ \mathbf{d}_t $ is a descent direction, then the
        angle $ \theta $ between $ \mathbf{d}_t $,
        $ -\nabla f(\mathbf{x}_{t - 1}) $ is in $ [0, \pi / 2) $, i.e.
        $ \theta $ is an acute angle.

        \item
        In practice, for $ \mathbf{H}_t \in \mathbb{R}^{n \times n} $,
        $ \mathbf{H}_t = \mathbf{H}_t^\top $, $ |\mathbf{H}_t| \ne 0 $, we
        often choose \cite{nocedal_opt}
        \begin{equation} \label{eq:gen_descent_step}
            \mathbf{d}_t = -\mathbf{H}_t^{-1}\nabla f(\mathbf{x}_{t - 1})
        \end{equation}
        Note when $ \mathbf{H}_t \succ \mathbf{0} $, $ \mathbf{d}_t^\top
        \nabla f(\mathbf{x}_{t - 1}) = -\nabla f(\mathbf{x}_{t - 1})^\top
        \mathbf{H}_t^{-1}\nabla f(\mathbf{x}_{t - 1}) < 0 $. In other words,
        $ \mathbf{H}_t \succ \mathbf{0} \Rightarrow \mathbf{d}_t $ is a
        descent direction.
    \end{itemize}
\end{frame}

\begin{frame}{Line search}
    \begin{itemize}
        \item
        Assuming $ \mathbf{d}_t $ computed, the optimal choice of $ \eta_t $
        is such that
        \begin{equation} \label{eq:exact_line_search}
            \eta_t = \arg\min_{\eta > 0}
            f(\mathbf{x}_{t - 1} + \eta\mathbf{d}_t)
        \end{equation}
        Too expensive to compute in general, but simply choosing $ \eta_t $
        s.t. $ f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) <
        f(\mathbf{x}_{t - 1}) $ does \textbf{not} guarantee convergence
        \cite{nocedal_opt}.

        \item
        Instead, require ``sufficient decrease'' in $ f $ each iteration.
        A common condition is the \textit{Armijo condition}, which for
        $ \alpha \in (0, 1) $, requires
        \begin{equation} \label{eq:armijo_rule}
            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) \le
            f(\mathbf{x}_{t - 1}) + \alpha\eta_t\mathbf{d}_t^\top
            \nabla f(\mathbf{x}_{t - 1})
        \end{equation}
        Note $ \mathbf{d}_t $ is a descent direction $ \Rightarrow
        \mathbf{d}_t^\top\nabla f(\mathbf{x}_{t - 1}) < 0 $.
        (\aref{eq:armijo_rule}) stipulates $ f(\mathbf{x}_{t - 1} +
        \eta_t\mathbf{d}_t) $ must fall under a damped linear approximation.

        \item
        For convex $ f $, $ \{\eta_t\}_{t \in \mathbb{N}} $ satisfying
        (\aref{eq:armijo_rule}) and descent directions
        $ \{\mathbf{d}_t\}_{t \in \mathbb{N}} $ \textbf{guarantee} that the
        global minimizer of $ f $ will be reached \cite{stat_learn_sparsity}.
    \end{itemize}
\end{frame}

%\begin{frame}{Line search}
%    \begin{itemize}
%        \item
%        Although (\aref{eq:armijo_rule}) is not sufficient to ensure that the
%        reasonable progress along $ \mathbf{d}_t $, it is sufficient if
%        $ \eta_t $ chosen using \textit{backtracking} \cite{nocedal_opt}.
%    \end{itemize}
%    \begin{centering}
%    \scalebox{0.9}{
%	    \begin{algorithm}[H]
%	        \KwIn{cheese}
%	        \caption{Backtracking line search}
%	    \end{algorithm}
%	}
%    \end{centering}
%\end{frame}

%\subsection{Gradient descent}
%
%\begin{frame}{Gradient descent}
%    \begin{itemize}
%        \item
%        \textit{Gradient descent} chooses $ \mathbf{d}_t = -\nabla f(\mathbf{x}_{t - 1}) $, i.e. $ \mathbf{H}_t = \mathbf{I} $. Consider a first-order Taylor approximation around $ \mathbf{x}_{t - 1} $ for small\footnote{
%            Again, in the sense that $ \eta_t\mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) $ dominates $ O(\eta_t^2) $ terms.
%        } $ \eta_t $, i.e.
%        \begin{equation*}
%            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}) \approx f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1})
%        \end{equation*}
%        The unit vector\footnote{
%            If we don't constrain $ \Vert\mathbf{d}\Vert_2 $, (\aref{eq:grad_descent_opt}) is unbounded below.
%        } $ \hat{\mathbf{d}} $ maximizing $ f(\mathbf{x}_{t - 1}) - f(\mathbf{x}_{t - 1} + \mathbf{d}) $ solves \cite{nocedal_opt}
%        \begin{equation} \label{eq:grad_descent_opt}
%            \begin{array}{ll}
%                \displaystyle\min_{\mathbf{d}} & \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) \\
%                \text{s.t.} & \Vert\mathbf{d}\Vert_2 = 1
%            \end{array}
%        \end{equation}
%
%        \item
%        Since $ \forall \mathbf{d} \in \mathbb{R}^n $, $ \Vert\mathbf{d}\Vert_2 = 1 $, $ \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) = \Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2\cos\theta $, solution to (\aref{eq:grad_descent_opt}) results in $ \cos\theta = -1 \Leftrightarrow \theta = \pi \Leftrightarrow \hat{\mathbf{d}} = -\frac{\nabla f(\mathbf{x}_{t - 1})}{\Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2} $.
%
%        \item
%        Gradient descent sometimes called the \textit{method of steepest descent} since $ \mathbf{d}_t \propto \hat{\mathbf{d}} $ giving the direction of greatest decrease in $ f $.
%    \end{itemize}
%
%    % spacing for footnotes
%    \medskip
%\end{frame}

%\subsection{Newton's method}
%
%\begin{frame}{Newton's method}
%    \begin{itemize}
%        \item
%        Suppose $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \nabla^2f(\mathbf{x}) \succ \mathbf{0} $. Consider the Taylor expansion
%        \begin{equation*}
%            \begin{split}
%            f(\mathbf{x}_{t - 1} + \eta_t\mathbf{d}_t) & =
%                f(\mathbf{x}_{t - 1}) + \eta_t\mathbf{d}_t^\top
%                \nabla f(\mathbf{x}_{t - 1}) + \frac{1}{2}\eta_t^2
%                \mathbf{d}_t^\top\nabla^2f(\mathbf{x}_{t - 1})\mathbf{d}_t \\
%                & \quad + O(\eta_t^3)
%            \end{split}
%        \end{equation*}
%        Keeping terms up to $ O(\eta_t^2) $, the estimate $ \hat{\mathbf{d}} $ for $ \eta_t\mathbf{d}_t $ solves
%        \begin{equation} \label{eq:newton_descent_opt}
%            \begin{array}{ll}
%                \displaystyle\min_{\mathbf{d}} & \mathbf{d}^\top\nabla f(\mathbf{x}_{t - 1}) + \frac{1}{2}\mathbf{d}^\top\nabla^2f(\mathbf{x}_{t - 1})\mathbf{d}
%            \end{array}
%        \end{equation}
%
%        \item
%        Since (\aref{eq:newton_descent_opt}) convex and $ \left|\nabla^2f(\mathbf{x}_{t - 1})\right| \ne 0 $, then $ \hat{\mathbf{d}} = \big(\nabla^2f(\mathbf{x}_{t - 1})\big)^{-1}\nabla f(\mathbf{x}_{t - 1}) $
%
%        \item
%        \textit{Newton's method} chooses $ \mathbf{d}_t = -\nabla^2f(\mathbf{x}_{t - 1})^{-1}\nabla f(\mathbf{x}_{t - 1}) $, i.e. $ \mathbf{H}_t = \nabla^2f(\mathbf{x}_{t - 1}) $. $ \exists\nabla^2f $ must exist else we cannot use this choice.
%
%        \item
%        $ \mathbf{d}_t $ is a descent direction $ \Leftrightarrow \forall \mathbf{x} \in \mathbb{R}^n $, $ \nabla^2f(\mathbf{x}) \succ \mathbf{0} $.
%    \end{itemize}
%\end{frame}


% BibTeX slide for references. should use either acm or ieeetr style
\begin{frame}{References}
    \bibliographystyle{acm}
    % relative path may need to be updated depending on .tex file location
    \bibliography{../master_bib}
\end{frame}

\end{document}