%%% exercise 07 %%%
\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, enumitem, fancyhdr, tikz}
% get rid of paragraph indent
\setlength{\parindent}{0 pt}
% allow section.equation numbering
\numberwithin{equation}{section}
% allows you to copy-paste code segments. requires pygments to be installed
% natively, i.e. not just in a Python venv! for example, on WSL Ubuntu you
% need to sudo apt-get install python3-pygments (unfortunately, the version
% will be a bit dated) and cannot rely on your venv Pygments version.
\usepackage{minted}
% makes clickable links to sections
\usepackage{hyperref}
% make the link colors blue, as well as cite colors. urls are magenta
\hypersetup{
    colorlinks, linkcolor = blue, citecolor = blue, urlcolor = magenta
}
% fancy pagestyle so we can use fancyhdr for fancy headers/footers
\pagestyle{fancy}
% add logo in right of header. note that you will have to adjust logo path!
\fancyhead[R]{\includegraphics[scale = 0.15]{../bac_logo1.png}}
% don't show anything in the left and center header
\fancyhead[L, C]{}
% give enough space for logo by reducing top margin height, head separator,
% increasing headerheight. see Figure 1 in the fancyhdr documentation. if
% \topmargin + \headheight + \headsep = 0, original text margins unchanged.
\setlength{\topmargin}{-60 pt}
\setlength{\headheight}{50 pt}
\setlength{\headsep}{10 pt}
% remove decorative line in the fancy header
\renewcommand{\headrulewidth}{0 pt}

% title, author + thanks, date
\title{Exercise 7}
\author{Derek Huang\thanks{NYU Stern 2021, BAC Advanced Team.}}
\date{June 6, 2021}

% shortcut links. the % characters strip extra spacing.
\newcommand{\pytest}{\href{https://docs.pytest.org/en/stable/}{pytest}}
\newcommand{\minimize}{%
    \href{%
        https://docs.scipy.org/doc/scipy/reference/generated/%
        scipy.optimize.minimize.html%
    }{\texttt{scipy.optimize.minimize}}%
}
% don't wanna keep typing this out
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\npinv}{%
    \href{%
        https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html%
    }{\texttt{numpy.linalg.inv}}%
}
\newcommand{\pdb}{%
    \href{https://docs.python.org/3/library/pdb.html}{\texttt{pdb}}%
}
\newcommand{\armijobacktrack}{\texttt{\_armijo\_backtrack}}
\newcommand{\nagsolver}{\texttt{nag\_solver}}

\begin{document}

\maketitle
% need to include this after making title to undo the automatic
% \thispagestyle{plain} command that is issued.
\thispagestyle{fancy}


\section{Introduction}

The goal of this exercise is to implement Nesterov's accelerated gradient
method, where step sizes can be either fixed or chosen by the Armijo rule
using backtracking line search.

\section{Instructions}

\subsection{General}

The \texttt{exercise\_07.py} file contains the \texttt{FastGradResult} class,
skeletons for the functions \armijobacktrack{} and \nagsolver, unit tests,
and \pytest{} fixtures. Complete the \armijobacktrack{} and \nagsolver{}
functions by replacing the \texttt{\#\#\# your code goes here \#\#\#} comment
blocks with appropriate Python code.

\medskip

Your code \textbf{must} be written in the areas marked by these blocks. Do
\textbf{not} change any of the pre-written code. The exercise is complete
$ \Leftrightarrow $ \texttt{pytest /path/to/exercise\_07.py} executes with
zero test failures.

\subsection{Implementing \armijobacktrack}

The \armijobacktrack{} method should compute a step size using the
backtracking line search algorithm discussed in the lecture, where the search terminates once the Armijo rule is satisfied. In this implementation, given a
previous parameter estimate $ \mathbf{x}_{t - 1} $, the descent direction
used should be $ -\nabla f(\mathbf{x}_{t - 1}) $, $ f $ the objective, so
$ \eta_t \leftarrow \gamma\eta_t $ until we satisfy the Armijo rule, that is
we have
\begin{equation*}
    f(\mathbf{x}_{t - 1} - \eta_t\nabla f(\mathbf{x}_{t - 1})) \le
    f(\mathbf{x}_{t - 1}) - \alpha\eta_t
    \Vert\nabla f(\mathbf{x}_{t - 1})\Vert_2^2
\end{equation*}

Here $ \alpha, \gamma $ correspond to the \texttt{arm\_alpha},
\texttt{arm\_gamma} parameters respectively. Since the negative gradient is
the descent direction, no descent direction is required as input to
\armijobacktrack. For convenience, use the variables \texttt{eta},
\texttt{fobj}, \texttt{fgrad} defined by the function skeleton in your code.

\subsection{Implementing \nagsolver}

The \nagsolver{} method should implement Nesterov's accelerated gradient
descent for differentiable objectives, i.e. there is no proximal step after
computing the gradient step using the extrapolation points
$ \{\tilde{\mathbf{x}}_t\}_{t \in \mathbb{N}} $. The comments in the
\nagsolver{} function skeleton given additional hints as to what you should
do. For convenience, use the variables \texttt{x}, \texttt{nest\_x}, and
\texttt{grad} defined in the \nagsolver{} skeleton, noting that
$ \forall t \in \mathbb{N} $, \texttt{x} should store the actual parameter
estimates $ \mathbf{x}_{t - 1} $, $ \mathbf{x}_t $ while \texttt{nest\_x}
should store the extrapolation points $ \tilde{\mathbf{x}}_{t - 1} $,
$ \tilde{\mathbf{x}}_t $. Remember that gradient steps should only be
performed on the extrapolation points
$ \{\tilde{\mathbf{x}}_t\}_{t \in \mathbb{N}} $, i.e. on \texttt{nest\_x}.
Also note that the learning rate $ \eta_t $ is controlled by the
\texttt{learning\_rate}, \texttt{arm\_alpha}, \texttt{arm\_gamma} parameters,
where \texttt{learning\_rate="constant"} $ \Rightarrow \forall t \in
\mathbb{N} $, $ \eta_t \triangleq \eta_0 $, while
\texttt{learning\_rate="backtrack"} $ \Rightarrow \eta_t $ is computed using
\armijobacktrack{} with \texttt{arm\_alpha=arm\_alpha} and
\texttt{arm\_gamma=arm\_gamma}.

\subsection{Tips}

\begin{enumerate}
    \item
    Carefully read the docstrings for \armijobacktrack{} and \nagsolver.
    Review slides 7 and 15.

    \item
    Remember that Nesterov's method keeps track of both the actual parameter
    estimates $ \{\mathbf{x}_t\}_{t \in \mathbb{N}} $ and the extrapolation
    points $ \{\tilde{\mathbf{x}}_t\}_{t \in \mathbb{N}} $, but that gradients
    are evaluated only at the extrapolation points.

    \item
    Use NumPy functions whenever possible for efficiency, brevity, and ease
    of debugging.

    \item
    Invoke \pytest{} with the \texttt{--pdb} flag to start \pdb, the Python
    debugger. Doing so allows you to inspect the variables in the current call
    frame and look more closely at what went wrong.
\end{enumerate}

\end{document}