% standard beamer lecture template for slides
% by Derek Huang
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e, amsmath, amssymb, amsfonts, graphicx}
% allow section.equation numbering
\numberwithin{equation}{section}
% use boadilla theme
\usetheme{Boadilla}
% remove navigation symbols
\usenavigationsymbolstemplate{}
% get numbered figure captions
\setbeamertemplate{caption}[numbered]
% changes itemize to circle + other things
\useoutertheme{split}
\useinnertheme{circles}

% command for the title string. change for each lecture
\newcommand{\lecturetitle}{Linear Regression}
% allow automatic alert-highlighted references, href
\newcommand{\aref}[1]{\alert{\ref{#1}}}
\newcommand{\ahref}[2]{\href{#1}{\alert{#2}}}
% title page stuff. brackets content displayed in footer bar
\title[\lecturetitle]{\lecturetitle}
% metadata. content in brackets is displayed in footer bar
\author[Derek Huang (BAC Advanced Team)]{Derek Huang}
\institute{BAC Advanced Team}
\date{February 27, 2021}

% change "ball" bullet to numbered bullet and section title for section
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
% change ball to gray square (copied from stackoverflow; \par needed for break)
\setbeamertemplate{subsection in toc}{        
    \hspace{1.2em}{\color{gray}\rule[0.3ex]{3pt}{3pt}}~\inserttocsubsection\par}
% use default enumeration scheme
\setbeamertemplate{enumerate items}[default]
% required line that fixes the problem of \mathbf, \bf not working in beamer
% for later (post-2019) TeX Live installations. see the issue on GitHub:
% https://github.com/josephwright/beamer/issues/630
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{}

\begin{document}

% title slide
\begin{frame}
    \titlepage
    \centering
    % relative path may need to be updated depending on .tex file location
    \includegraphics[scale = 0.1]{../bac_logo1.png}
\end{frame}

% table of contents slide
\begin{frame}{Overview}
    \tableofcontents
\end{frame}

\begin{frame}{Motivation}
    \begin{itemize}
        \item
        The simplest\footnote{
            It may seem simple, but it contains more subtleties than one
            might expect.
        } regression method that one should understand.

        \item
        \textit{Remark.} Simple $ \ne $ not effective.

        \item
        The fundamental basis for understanding more ``interesting'' models.

        \item
        Methods used to fit the model in practice are less expensive in both
        CPU time and memory usage compared to other models.

        \item
        How much about the model do you \textit{really} understand?
    \end{itemize}
\end{frame}

\section{Linear regression}

\subsection{The linear model}

\begin{frame}{The linear model}
    \begin{itemize}
        \item
        Suppose on the real-world $ (\Omega, \mathcal{F}, \mathbb{P}) $ we
        have random variables $ X : \Omega \rightarrow \mathbb{R}^d $,
        $ Y : \Omega \rightarrow \mathbb{R} $. The \textit{linear regression
        model}\footnote{
            Technically, it should be called \textit{affine} regression since
            $ b $ provides a shift.
        } posits that
        \begin{equation} \label{lr_model}
            Y = \mathbf{w}^\top X + b + \varepsilon
        \end{equation}
        Here $ \mathbf{w} \in \mathbb{R}^d $, $ b \in \mathbb{R} $, and
        independent $ \varepsilon \sim \mathcal{N}(0, \sigma^2) $\footnote{
            The assumption on homoscedastic Gaussian errors will be discussed
            next lecture.
        }, $ \sigma \in (0, \infty) $.

        \item
        Interpretation is that $ Y $ can be written as a translated linear
        combination of the $ X $ components plus an independent noise term.

        \item
        The perennial question: Assuming true $ \mathbf{w}, b $ exists, how do
        we estimate $ \mathbf{w}, b $ sufficiently well given training data
        $ \mathcal{D} $?
    \end{itemize}
\end{frame}

\subsection{Solving the system}

\begin{frame}{Solving the system}
    \begin{itemize}
        \item
        Suppose $ \mathcal{D} \triangleq (\mathbf{X}, \mathbf{y}) $, where
        $ \mathbf{X} \in \mathbb{R}^{N \times d} $ is the input matrix, each
        row a transposed data example, $ \mathbf{y} \in \mathbb{R}^N $ the
        response vector. Clearly the problem we have is that of finding
        $ \hat{\mathbf{w}} \in \mathbb{R}^d , \hat{b} \in \mathbb{R} $
        such that
        \begin{equation*}
            \mathbf{X}_\mathbf{1}\hat{\mathbf{w}}_{\hat{b}} \triangleq
            \begin{bmatrix}
                \ \mathbf{X} & \mathbf{1} \
            \end{bmatrix}
            \begin{bmatrix}
                \ \hat{\mathbf{w}} \ \\ \ \hat{b} \
            \end{bmatrix} \approx \mathbf{y}
        \end{equation*}
        Here $ \mathbf{X}_\mathbf{1} \in \mathbb{R}^{N \times (d + 1)} $,
        $ \hat{\mathbf{w}}_{\hat{b}} \in \mathbb{R}^{d + 1} $. Assume
        $ \operatorname{rk}(\mathbf{X}_\mathbf{1}) = \min\{N, d + 1\} $.

        \item
        Problem: often $ N \ne d + 1 $, so $ \mathbf{X}_\mathbf{1} $ is
        \textbf{not} invertible, and therefore $ \nexists
        \hat{\mathbf{w}}_{\hat{b}} $ such that $ \hat{\mathbf{w}}_{\hat{b}} =
        \mathbf{X}_\mathbf{1}^{-1}\mathbf{y} $, i.e. $ \mathbf{X}_1
        \hat{\mathbf{w}}_{\hat{b}} = \mathbf{y} $. But consider
        \begin{equation} \label{lr_2norm_aug}
            \begin{array}{ll}
                \displaystyle\min_{\mathbf{w}_b} &
                \Vert\mathbf{y} - \mathbf{X}_\mathbf{1}\mathbf{w}_b\Vert_2^2
            \end{array}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}{Solving the system}
    \begin{itemize}
        \item
        Cast solving of an over/under-determined linear system as minimization
        of the $ \ell^2 $-norm of the residual vector $ \mathbf{r} \triangleq
        \mathbf{y} - \mathbf{X}_\mathbf{1}\hat{\mathbf{w}}_{\hat{b}} $.

        \item
        \textit{Remark.} (\aref{lr_2norm_aug}) minimizes the \textbf{squared}
        $ \ell^2 $-norm of $ \mathbf{r} $, but note that
        \begin{equation*}
            \arg\min_{\mathbf{w}_b}
            \Vert\mathbf{y} - \mathbf{X}_\mathbf{1}\mathbf{w}_b\Vert_2 =
            \arg\min_{\mathbf{w}_b}
            \Vert\mathbf{y} - \mathbf{X}_\mathbf{1}\mathbf{w}_b\Vert_2^2
        \end{equation*}
        For function $ f $, $ \arg\min_\mathbf{x}f(\mathbf{x}) $ retrieves
        the point that minimizes $ f $.

        \item
        \textit{Remark.} (\aref{lr_2norm_aug}) minimizes the
        \textbf{sum of squared residuals}. Also, note that
        $ \Vert\mathbf{y} - \mathbf{X}_\mathbf{1}\mathbf{w}_b\Vert_2^2 =
        (\mathbf{y} - \mathbf{X}_\mathbf{1}\mathbf{w}_b)^\top(\mathbf{y} -
        \mathbf{X}_\mathbf{1}\mathbf{w}_b) $ by definition.

        \item
        \textit{Properties of }(\aref{lr_2norm_aug}). Let $ f(\mathbf{w}_b) =
        \Vert\mathbf{y} - \mathbf{X}_\mathbf{1}\mathbf{w}_b\Vert_2^2 $.
        \begin{enumerate}
            \item
            $ \forall \mathbf{w}_b \in \mathbb{R}^{d + 1} $,
            $ \exists \nabla f(\mathbf{w}_b) $ (differentiable everywhere).

            \item
            $ f $ convex $ \Rightarrow \hat{\mathbf{w}}_{\hat{b}} $ optimal
            $ \Leftrightarrow \nabla f(\hat{\mathbf{w}}_{\hat{b}}) =
            \mathbf{0} $ ($ \exists $ global minimum) \cite{bv_convex_opt}.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}{Solving the system}
    \begin{itemize}
        \item
        At optimal $ \hat{\mathbf{w}}_{\hat{b}} \in \mathbb{R}^{d + 1} $,
        $ \nabla \Vert\mathbf{y} - \mathbf{X}_\mathbf{1}
        \hat{\mathbf{w}}_{\hat{b}}\Vert_2^2 = \mathbf{0} $. See that
        \begin{equation*}
            \nabla\Vert\mathbf{y} -
            \mathbf{X}_\mathbf{1}\hat{\mathbf{w}}_{\hat{b}}\Vert_2^2 =
            -2\mathbf{X}_\mathbf{1}^\top(\mathbf{y} -
            \mathbf{X}_\mathbf{1}\hat{\mathbf{w}}_{\hat{b}}) = \mathbf{0}
            \Leftrightarrow \mathbf{X}_\mathbf{1}^\top\mathbf{y} =
            \mathbf{X}_\mathbf{1}^\top\mathbf{X}_\mathbf{1}
            \hat{\mathbf{w}}_{\hat{b}}
        \end{equation*}

        \item
        $ \mathbf{X}_\mathbf{1}^\top\mathbf{X}_\mathbf{1} \in
        \mathbb{R}^{(d + 1) \times (d + 1)} $ is full rank\footnote{
            See first answer of this \ahref{
https://math.stackexchange.com/questions/349738/prove-operatornamerankata-operatornameranka-for-any-a-in-m-m-times-n}
            {math.stackexchange} question for proof.
        }, i.e. invertible\footnote{
            In practice this inverse is rarely computed. For example,
            \ahref{
https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html
            }{\texttt{scipy.linalg.lstsq}} LAPACK's \ahref{
http://www.netlib.org/lapack/explore-html/db/d6a/dgelsd_8f.html
            }{\texttt{gelsd}} by default, which uses the singular value
            decomposition.
        },
        and therefore
        \begin{equation} \label{lr_ols_aug}
            \hat{\mathbf{w}}_{\hat{b}} = \big(
                \mathbf{X}_\mathbf{1}^\top\mathbf{X}_\mathbf{1}
            \big)^{-1}\mathbf{X}_\mathbf{1}^\top\mathbf{y}
        \end{equation}
        Or, written in terms of the original $ \hat{\mathbf{w}}, \hat{b} $,
        \begin{equation} \label{lr_ols_block}
            \begin{bmatrix}
                \ \hat{\mathbf{w}} \ \\ \ \hat{b} \
            \end{bmatrix} =
            \left(
                \begin{bmatrix}
                    \ \mathbf{X}^\top \ \\ \ \mathbf{1}^\top \
                \end{bmatrix}
                \begin{bmatrix}
                    \ \mathbf{X} & \mathbf{1} \
                \end{bmatrix}
            \right)^{-1}
            \begin{bmatrix}
                \ \mathbf{X} & \mathbf{1} \
            \end{bmatrix}^\top\mathbf{y}
        \end{equation}
    \end{itemize}
\end{frame}

\subsection{On centered data}

\begin{frame}{On centered data}
    \begin{itemize}
        \item
        Consider: can we estimate $ \mathbf{w}, b $ separately?

        \item
        \textit{Proposition.} Let $ X : \Omega \rightarrow \mathbb{R}^d $,
        $ Y : \Omega \rightarrow \mathbb{R} $ be random variables on
        $ (\Omega, \mathcal{F}, \mathbb{P}) $, and let $ \varepsilon \sim
        \mathcal{N}(0, \sigma^2) $ be independent of $ X $, $ \sigma \in
        (0, \infty) $. For $ \mathbf{w} \in \mathbb{R}^d $,
        $ b \in \mathbb{R} $,
        $ Y = \mathbf{w}^\top X + b + \varepsilon \Rightarrow b = \mu_Y -
        \mathbf{w}^\top\mu_X $.

        \item
        \textit{Proof.} Taking expectations, $ \mu_Y = \mathbf{w}^\top\mu_X +
        b \Leftrightarrow b = \mu_Y - \mathbf{w}^\top\mu_X $.

        \item
        \textit{Remark.} For any model $ Y = f(X) + b + \varepsilon $,
        $ b = \mu_Y - \mu_{f(X)} $.

        \item
        \textit{Corollary.} $ Y - \mu_Y = \mathbf{w}^\top(X - \mu_X) +
        \varepsilon $.

        \item
        \textit{Proof.} $ b = \mu_Y - \mathbf{w}^\top\mu_X \Rightarrow
        Y - \mu_Y = \mathbf{w}^\top(X - \mu_X) + \varepsilon $.

        \item
        Conclusion: $ \mathbf{w} $ can be estimated independently of $ b $ if
        inputs and outputs are centered before being used to estimate $ b $.
    \end{itemize}        
\end{frame}

\begin{frame}{On centered data}
    \begin{itemize}
        \item
        Let $ \mathbf{X} \in \mathbb{R}^{N \times d} $ be the input matrix,
        $ \mathbf{y} \in \mathbb{R}^N $ be the response vector. Let
        $ \bar{\mathbf{x}} \triangleq \frac{1}{N}\mathbf{X}^\top\mathbf{1} $,
        $ \bar{y} \triangleq \frac{1}{N}\mathbf{y}^\top\mathbf{1} $,
        $ \tilde{\mathbf{X}} \triangleq \mathbf{X} -
        \mathbf{1}\bar{\mathbf{x}}^\top $, $ \tilde{\mathbf{y}} \triangleq
        \mathbf{y} - \bar{y}\mathbf{1} $.

        \item
        Consider the no-intercept problem $ \min_{\mathbf{w}}\Vert
        \tilde{\mathbf{y}} - \tilde{\mathbf{X}}\mathbf{w}\Vert_2^2 $. Assuming
        that $ \operatorname{rk}\big(\tilde{\mathbf{X}}\big) = \min\{N, d\} $,
        from (\aref{lr_ols_aug}) it's clear that $ \hat{\mathbf{w}} = \big(
        \tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\big)^{-1}
        \tilde{\mathbf{X}}^\top\tilde{\mathbf{y}} $.

        \item
        However, $ \tilde{\mathbf{X}} $ centered $ \Rightarrow
        \tilde{\mathbf{X}}^\top\mathbf{1} = \mathbf{0} \Rightarrow
        \tilde{\mathbf{X}}^\top\tilde{\mathbf{y}} =
        \tilde{\mathbf{X}}^\top\mathbf{y} - \bar{y}
        \tilde{\mathbf{X}}^\top\mathbf{1} =
        \tilde{\mathbf{X}}^\top\mathbf{y} $. We can therefore replace
        $ \tilde{\mathbf{y}} $ with $ \mathbf{y} $ in this case and see that
        \begin{equation} \label{lr_ols_cent}
            \begin{split}
                \hat{\mathbf{w}} & = \big(
	                \tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}
	            \big)^{-1}\tilde{\mathbf{X}}^\top\mathbf{y} \\
	            \hat{b} & = \bar{y} - \hat{\mathbf{w}}^\top\bar{\mathbf{x}}
            \end{split}
        \end{equation}
        This will also be true for \textit{ridge regression}.
    \end{itemize}
\end{frame}

\section{Ridge regression}

\begin{frame}

\end{frame}

% BibTeX slide for references. should use either acm or ieeetr style
\begin{frame}{References}
    \bibliographystyle{acm}
    % relative path may need to be updated depending on .tex file location
    \bibliography{../master_bib}
\end{frame}

\end{document}