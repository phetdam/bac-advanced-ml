%%% lecture 01 %%%
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts, graphicx}
% use boadilla theme
\usetheme{Boadilla}
% remove navigation symbols
\usenavigationsymbolstemplate{}
% get numbered figure captions
\setbeamertemplate{caption}[numbered]
% changes itemize to circle + other things
\useoutertheme{split}
\useinnertheme{circles}

% allow section.equation numbering
\numberwithin{equation}{section}

% title page stuff
\title[Math for Data Science, Part 1]{Math for Data Science, Part 1}
\author[Derek Huang (BAC Advanced Team)]{Derek Huang}
\institute{BAC Advanced Team}
\date{February 19, 2021}

% change "ball" bullet to numbered bullet and section title for section
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
% change ball to gray square (copied from stackoverflow; \par needed for break)
\setbeamertemplate{subsection in toc}{        
    \hspace{1.2em}{\color{gray}\rule[0.3ex]{3pt}{3pt}}~\inserttocsubsection\par}
% use default enumeration scheme
\setbeamertemplate{enumerate items}[default]
% required line that fixes the problem of \mathbf, \bf not working in beamer
% for later (post-2019) TeX Live installations. see the issue on GitHub:
% https://github.com/josephwright/beamer/issues/630
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{}

\begin{document}

% title slide
\begin{frame}
    \titlepage
    \centering
    \includegraphics[scale = 0.1]{../bac_logo1.png}
\end{frame}

% table of contents
\begin{frame}{Overview}
	\tableofcontents
\end{frame}

\section{Introduction}

\subsection{Motivation}

\begin{frame}{Motivation}
    \begin{itemize}
        \item
        Why do we need a math review/primer?
        
        \item
        Short answer: so that we can be \textit{informed} model users.
        
        \item
        Uninformed use often gives \textit{seriously misleading} modeling
        results.

        \item
        \textit{Example}. Principal components analysis (PCA). A typical
        mistake people make\footnotemark\footnotetext{
            I did this before but caught my mistake before I turned in my
            analysis.
        } is to perform PCA on data that has not been scaled to have unit
        per-component variances.

        \item
        Since PCA is scaling sensitive, one might get a couple massive
        and many small eigenvalues/singular values\footnotemark\footnotetext{
            Define $ \mathbf{X} \in \mathbb{R}^{N \times d} $ as the data
            matrix with sample covariance matrix $ \mathbf{C} \triangleq
            \frac{1}{N}\mathbf{X}^\top\mathbf{X} $. Eigendecomposition is used
            on $ \mathbf{C} $ while singular value decomposition is used on
            $ \mathbf{X} $.
        } and incorrectly conclude that data is well approximated by a
        low-dimension orthogonal eigenbasis.
    \end{itemize}
\end{frame}

\subsection{Topics}

\begin{frame}{Topics}
    \begin{itemize}
        \item
        It can only help and never hurt to know more
        math\footnotemark\footnotetext{
            Interestingly, \href{
                https://cseweb.ucsd.edu/~yfreund/papers/brownboost.pdf
            }{\alert{Freund's BrownBoost}} algorithm has connections to
            Brownian motion and requires the solution to a differential
            equation at each iteration.
        }, but our time is
        limited. Therefore, we will only briefly cover enough mathematical
        knowledge to give a useful foundation for self-learning.

        \item
        In general, knowing all the following topics at an undergraduate or
        early graduate level is ideal: probability theory, linear algebra,
        vector/matrix calculus, and some convex/nonlinear optimization.

        \item
        It goes without mentioning that an elementary understanding of set
        theory is necessary. Knowing some real analysis is helpful.
    \end{itemize}
\end{frame}

\section{Set theory}

\subsection{Set basics}

\begin{frame}{Set basics}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{set} is a collection of non-duplicate
        items.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Sample space of coin flip.} H for heads, T for tails. The
            sample space $ \Omega \triangleq \{\text{H}, \text{T}\} $, where
            $ \triangleq $ means ``is defined as''.

            \item
            \textit{Natural numbers.} A special set, denoted with
            $ \mathbb{N} $. The set of numbers $ 1, 2, $ etc. This is a
            \textit{countably infinite} set, as it has an infinite number of
            elements, but the elements can be ``counted out''.

            \item
            \textit{Real numbers.} A special set, denoted with $ \mathbb{R} $.
            This type of set if \textit{uncountably infinite}, as you can't
            ``count out'' all the real numbers.

            \item
            \textit{Open Euclidean ball}. $ B(\mathbf{x}, r) \triangleq
            \{\mathbf{x}' \in \mathbb{R}^n : \Vert\mathbf{x}' - \mathbf{x}
            \Vert_2 < r\} $, where $ \in $ means ``in'' and
            $ r \in (0, \infty) $. A \textit{closed ball} replaces $ < $ with
            $ \le $.
        \end{itemize}

        \item
        \textit{Definition.} The \textit{cardinality} of a set $ A $, denoted
        by $ |A| $, gives the number of elements in the set. Cardinality is
        mostly used in the context of finite sets---infinite sets are a
        different story.
    \end{itemize}
\end{frame}

\begin{frame}{Set basics}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{subset} $ B $ of a set $ A $ is a set
        containing only some elements from $ A $. Usually $ B \subseteq A $
        means that $ B $ might be equal to $ A $ while $ B \subset A $ means
        that $ B $ is a \textit{strict} subset of $ A $.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Empty set.} $ \emptyset $, a set containing nothing. For
            any set $ A $, $ \emptyset \subseteq A $.

            \item
            \textit{Power set.} Denote $ \Omega \triangleq \{\text{H},
            \text{T}\} $ as the coin flip sample space. The power set
            associated with $ \Omega $, the set of all possible subsets of
            $ \Omega $, is denoted with $ 2^\Omega \triangleq \{\emptyset,
            \{\text{H}\}, \{\text{T}\}, \{\text{H}, \text{T}\}\} $.
        \end{itemize}

        \item
        \textit{Definition.} For sets $ A, B $, $ A \cap B $ denotes the 
        \textit{intersection} of $ A, B $, i.e. the set that contains only
        elements common to $ A, B $.

        \item
        \textit{Definition.} For sets $ A, B $, $ A \cup B $ denotes the
        \textit{union} of $ A, B $, i.e. the set containing all the elements
        of $ A, B $.
    \end{itemize}
\end{frame}

\begin{frame}{Set basics}
    \begin{itemize}
        \item
        \textit{Definition.} Two sets $ A, B $ are \textit{disjoint} if
        $ A \cap B = \emptyset $.

        \item
        \textit{Definition.} Sets $ A_1, A_2 \ldots $ are
        \textit{mutually disjoint} if $ \bigcap_{k = 1}^\infty A_k =
        \emptyset $.

        \item
        \textit{Definition.} For sets $ A, B $, the \textit{set difference}
        $ A \setminus B $ is the set containing all elements in $ A $ that are
        not in $ B $.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            $ \mathbb{R} \setminus [0, \infty) = (-\infty, 0) $, the set of
            negative real numbers. 

            \item
            $ [0, \infty) \setminus \mathbb{R} = \emptyset $. Set difference
            is \textit{not} a symmetric relation.

            \item
            Denote $ \mathbb{Q} $ as the set of rationals.
            $ \mathbb{R} \setminus \mathbb{Q} $ is the set of irrationals.
        \end{itemize}

        \item
        \textit{Definition.} Denote $ \Omega $ as the \textit{universal set}
        and define $ A \subseteq \Omega $. The \textit{complement} of $ A $
        with respect to $ \Omega $ is $ A^\mathsf{c} \triangleq \Omega
        \setminus A $.

        \item
        \textit{Definition.} The \textit{Cartesian product} of sets
        $ X_1, \ldots X_n $, $ \mathcal{C} \triangleq
        X_1 \times \ldots X_n $, is the set of all $ n $-tuples whose
        $ i $th element is an element of $ X_i $.
    \end{itemize}
\end{frame}

\begin{frame}{Set basics}
    \begin{itemize}
        \item
        \textit{Definition.} A  $ \sigma $\textit{-algebra}, or
        $ \sigma $\textit{-field}, on a set $ \Omega $ is a collection
        $ \mathcal{F} $ of subsets of $ \Omega $ that has the following
        properties \cite{tamuz_prob}:
        \begin{enumerate}
            \item
            $ \Omega \in \mathcal{F} $.

            \item
            \textit{Closure under complement.} $ A \in \mathcal{F} \Rightarrow
            A^\mathsf{c} \in \mathcal{F} $. $ \Rightarrow $ means ``implies''.
            \begin{itemize}
                \item
                Note that since $ \Omega \in \mathcal{F} $, then clearly
                $ \emptyset \in \mathcal{F} $.
            \end{itemize}

            \item
            \textit{Closed under countable unions.} $ A_1, A_2, \ldots \in
            \mathcal{F} \Rightarrow \bigcup_{k = 1}^\infty
            A_k \in \mathcal{F} $.

            \item
            \textit{Closed under countable intersections.} $ A_1, A_2,
            \ldots \in \mathcal{F} \Rightarrow \bigcap_{k = 1}^\infty A_k
            \in \mathcal{F} $.
        \end{enumerate}

        \item
        \textit{Definition.} A \textit{function} $ f : X \rightarrow Y $ is a
        mapping from a set $ X $ to a set $ Y $. We may also write
        $ f \triangleq \{(x, y) \in X \times Y : y = f(x)\} $, calling $ f $
        the set of ordered pairs $ (x, y) \in X \times Y $ where $ y = f(x) $.

        \item
        \textit{Definition.} The \textit{domain} of $ f $ is $ X $, also
        denoted as $ \operatorname{dom} f $, while the \textit{codomain} of
        $ f $ is $ Y $. The \textit{image} or \textit{range} of $ f $, denoted
        as $ \operatorname{im} f $ or $ f(X) $, is defined such that
        $ \operatorname{im} f \triangleq f(X) = \{f(x) \in Y : x \in X\} $.
    \end{itemize}
\end{frame}

\section{Probability}

\subsection{Probability basics}

\begin{frame}{Probability basics}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{sample space} is the universal set
        $ \Omega $ of outcomes.

        \item
        \textit{Definition.} A set $ A \subseteq \Omega $, $ \Omega $ a sample
        space, is an \textit{event}.

        \item
        \textit{Definition.} A \textit{probability measure} $ \mathbb{P} $
        defined on a sample space $ \Omega $ is a function from a
        $ \sigma $-algebra $ \mathcal{F} \subseteq 2^\Omega $ to the unit
        interval.

        \item
        \textit{Definition.} A \textit{probability space} is the 3-tuple
        $ (\Omega, \mathcal{F}, \mathbb{P}) $. $ \Omega $ is the universal set,
        $ \mathcal{F} $ is a $ \sigma $-algebra, and $ \mathbb{P} $ is a
        probability measure.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Probability space of a fair coin.}
            $ (\Omega, \mathcal{F}, \mathbb{P}) $, where $ \Omega \triangleq
            \{\text{H}, \text{T}\} $, $ \mathcal{F} \triangleq \{\emptyset,
            \{\text{H}\}, \{\text{T}\}, \Omega\} = 2^\Omega $.
            $ \mathbb{P}(\{\text{H}\}) = \mathbb{P}(\{T\}) = 1 / 2 $ since the
            coin is fair. Note that $ \mathbb{P}(\emptyset) = 0 $,
            $ \mathbb{P}(\Omega) = 1 $. Why?

            \item
            \textit{Probability space of 3-sided die.} $ (\Omega, \mathcal{F},
            \mathbb{P}) $, where $ \Omega \triangleq \{1, 2, 3\} $,
            $ \mathcal{F} \triangleq \{\emptyset, \{1\}, \{2\}, \{3\},
            \{1, 2\}, \{1, 3\}, \{2, 3\}, \Omega\} $. If $ A \triangleq $
            ``roll an odd number'', $ A = \{1, 3\} $ and
            $ \mathbb{P}(A) = \mathbb{P}(\{1\}) + \mathbb{P}(\{3\}) = 2 / 3 $.
            Why?
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Probability basics}
    \begin{itemize}
        \item
        \textit{Kolmogorov axioms} \cite{wasserman_stats}.
        \begin{enumerate}
            \item
            $ \forall A \in \mathcal{F} $, $ \mathbb{P}(A) \ge 0 $, i.e. any
            event has a nonnegative probability.

            \item
            $ \mathbb{P}(\Omega) = 1 $, i.e. the probability that anything
            happens is 1.

            \item
            For countably infinite, disjoint $ A_1, A_2, \ldots $, then
            \begin{equation*}
                \mathbb{P}\left(\bigcup_{k = 1}^\infty A_k\right) =
                \sum_{k = 1}^\infty\mathbb{P}(A_k)
            \end{equation*}
            The probability of the union of mutually disjoint events is equal
            to the sum of the individual events' probabilities.
        \end{enumerate}

        \item
        \textit{Consequences of the axioms.}
        \begin{enumerate}
            \item
            $ \mathbb{P}(\emptyset) = 1 - \mathbb{P}(\Omega) = 0 $.

            \item
            $ \forall A, B \subseteq \Omega $, $ \mathbb{P}(A \cup B) =
            \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) $. $ \forall $
            means ``for all''.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}{Probability basics}
    \begin{itemize}
        \item
        \textit{Definition.} Events $ A, B \subseteq \Omega $
        \textit{independent} $ \Leftrightarrow \mathbb{P}(A \cap B) =
        \mathbb{P}(A)\mathbb{P}(B) $. $ \Leftrightarrow $ means ``if and only
        if''. Independent $ A, B $ often denoted as $ A \perp B $.

        \item
        \textit{Remark.} Mutual exclusion \textbf{does not} imply independence,
        and vice versa. Suppose $ \Omega = \{\text{H}, \text{T}\} $. Note that
        $ \{\text{H}\} \cap \{\text{T}\} = \emptyset $, as the coin is not a
        quantum coin, and that $ \mathbb{P}(\emptyset) = 0 \ne 1 / 4 =
        \mathbb{P}(\{\text{H}\})\mathbb{P}(\{\text{T}\}) $.

        \item
        \textit{Definition.} For $ A, B \subseteq \Omega $,
        $ \mathbb{P}(B) \ne 0 $, define
        \begin{equation*}
            \mathbb{P}(A \mid B) \triangleq \frac{\mathbb{P}(A \cap B)}{
                \mathbb{P}(B)            
            }
        \end{equation*}
        $ \mathbb{P}(A \mid B) $ is the \textit{conditional probability} of
        $ A $ given $ B $.

        \item
        \textit{Remark.} Conditional probabilities are \textbf{not}
        well-defined when $ \mathbb{P}(B) = 0 $. Suppose $ B = \emptyset $.
        Then, $ \mathbb{P}(B) = 0 $, and $ \forall A \in \mathcal{F} $,
        $ \mathbb{P}(A \cap B) = 0 $. Intuitively
        $ \mathbb{P}(A \mid B) = 0 $, but the definition breaks.
    \end{itemize}
\end{frame}

\subsection{Random variables}

\begin{frame}{Random variables}
    \begin{itemize}
        \item
        \textit{Definition.} A \textit{random variable} is a function
        $ X : \Omega \rightarrow \mathbb{F} $, $ \mathbb{F} $ a
        set\footnotemark\footnotetext{
            This set should be a \textit{field}, hence the $ \mathbb{F} $
            notation, but this is beyond our scope.        
        }. Usually $ \mathbb{F} $ is either $ \mathbb{R} $,
        $ \mathbb{R}^n $, or a subset of either.

        \item
        \textit{Definition.} The \textit{cumulative distribution function},
        or \textit{cdf}, of a random variable $ X $, is the function
        $ F_X : \mathbb{F} \rightarrow [0, 1] $ such that for
        $ x \in \mathbb{F} $,
        \begin{equation*}
            F_X(x) \triangleq \mathbb{P}\{X \preceq x\}
        \end{equation*}
        Here $ \preceq $ is a \textit{generalized inequality} inducing a
        partial order on $ \mathbb{F} $.

        \item
        \textit{Definition.} A random variable $ X $ is \textit{discrete} if
        $ X(\Omega) $ countable. Its associated \textit{probability mass
        function}, or \textit{pmf}, is the function
        $ p_X : \mathbb{F} \rightarrow [0, 1] $ such that
        $ \forall x \in \mathbb{F}, p_X(x) \triangleq \mathbb{P}\{X = x\} $.

        \item
        \textit{Example.}
        \begin{itemize}
            \item
            \textit{Coin flip payoff.} $ \Omega \triangleq \{\text{H},
            \text{T}\} $. Define $ X : \Omega \rightarrow \mathbb{R} $ where
            for $ q_1, q_2 > 0 $, $ X(\omega) \triangleq q_1
            \mathbb{I}_{\{\text{H}\}}(\omega) - 
            q_2\mathbb{I}_{\{\text{T}\}}(\omega) $. $ \mathbb{I}_A $ is the
            indicator function of a set $ A $, where $ \mathbb{I}_A(x) = 1 $
            if $ x \in A $, $ \mathbb{I}_A(x) = 0 $ if $ x \notin A $.
            $ X(\Omega) = \{-q_2, q_1\} $.
        \end{itemize}
    \end{itemize}

    % adjust spacing to we don't overlap footnote
    \medskip

\end{frame}

\begin{frame}{Random variables}
    \begin{itemize}
        \item
        \textit{Definition.} A random variable $ X $ is \textit{continuous} if
        $ \exists f_X : \mathbb{F} \rightarrow [0, \infty) $ with
        $ \int_\mathbb{F}f_X(x)\,dx = 1 $ such that $ \forall A \subseteq
        \mathbb{F} $,
        \begin{equation*}
            \mathbb{P}\{X \in A\} \triangleq \int_Af_X(x)\,dx
        \end{equation*}

        % adjust vertical spacing a bit
        \vspace{-5 pt}

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Exponential distribution.}
            $ X : \Omega \rightarrow \mathbb{R} $ has density
            $ f_X : \mathbb{R} \rightarrow [0, \infty) $ \\ such that
            $ \forall x \in \mathbb{R} $, $ \lambda \in (0, \infty) $,
            $ f_X(x) \triangleq \mathbb{I}_{[0, \infty)}(x)
            \lambda e^{-\lambda x} $.

            \item
            \textit{Multivariate Gaussian distribution.}
            $ X : \Omega \rightarrow \mathbb{R}^n $, $ X $ has density
            $ f_X : \mathbb{R}^n \rightarrow [0, \infty) $ such that
            $ \forall \mathbf{x} \in \mathbb{R}^n $, $ \mu \in \mathbb{R}^n $,
            $ \mathbf{\Sigma} \succ \mathbf{0} \in \mathbb{R}^{n \times n} $,
            \begin{equation*}
                f_X(\mathbf{x}) \triangleq
                \frac{1}{(2\pi)^{n / 2}|\mathbf{\Sigma}|^{1 / 2}}
                e^{
                    -\frac{1}{2}(\mathbf{x} - \mu)^\top\mathbf{\Sigma}^{-1}
                    (\mathbf{x} - \mu)
                }
            \end{equation*}
            $ \succ $ means ``positive definite'', $ |\cdot| $ the matrix
            determinant.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Random variables}
    \begin{itemize}
        \item
        \textit{Definition.} A finite collection of random variables
        $ X_1, \ldots X_n $, $ X_k : \Omega \rightarrow \mathbb{F} $, is
        \textit{mutually independent} $ \Leftrightarrow \forall A_1, \ldots
        A_n \subseteq \mathbb{F} $,
        \begin{equation*}
            \mathbb{P}\{X_1 \in A_1, \ldots X_n \in A_n\} =
            \prod_{k = 1}^n\mathbb{P}\{X_k \in A_k\}
        \end{equation*}
        The above definition holds if we replace $ \mathbb{P} $ with density
        functions \cite{wasserman_stats}.

        \item
        \textit{Example.}
        \begin{itemize}
            \item
            \textit{Independent uniforms.} Suppose $ U_1, \ldots U_n $
            mutually independent, $ U_k \sim \operatorname{Uniform}(0, 1) $.
            The density $ f_{U_k} $ for each $ U_k $ is such that
            \begin{equation*}
                f_{U_k}(u) = \mathbb{I}_{[0, 1]}(u)
            \end{equation*}
            Note $ f_{U_1, \ldots U_n}(u_1, \ldots u_n) =
            \mathbb{I}_{[0, 1]^n}(u_1, \ldots u_n) =
            \prod_{k = 1}^nf_{U_k}(u_k) $.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Random variables}
	\begin{itemize}
	    \item
        \textit{Definition.} A collection of random variables
        $ X_1, X_2, \ldots $ is \textit{independent and identically
        distributed}, or \textit{i.i.d.}, if $ X_1, X_2 \ldots $ are mutually
        independent and $ \forall k, X_k \sim Q $, $ Q $ representing some
        distribution.

        \item
        \textit{Examples.}
        \begin{itemize}
            \item
            \textit{Number of 6s rolled.} Suppose $ \Omega \triangleq
            \{1, 2, 3, 4, 5, 6\}^n $, $ n \in \mathbb{N} $, i.e. the set of
            all finite sequences of $ n $ fair die rolls. Let
            $ Y_n \triangleq \sum_{k = 1}^nX_k $, where $ X_k $ i.i.d. and
            $ X_k $ is $ 1 $ if the $ k $th roll is a $ 6 $, $ 0 $ otherwise.
            Note that $ \mathbb{P}\{X_k = 6\} = 1 / 6 $, so
            $ X_k \sim \operatorname{Bernoulli}(1 / 6) $,
            $ Y_n \sim \operatorname{Binomial}(n, 1 / 6) $.

            \item
            \textit{i.i.d. Gaussians.} Let $ Z_1, \ldots Z_n $ i.i.d.,
            $ Z_k \sim \mathcal{N}(0, 1) $. If we let
            $ X \triangleq [ \ Z_1 \ \ldots \ Z_n \ ]^\top $, then
            $ f_X : \mathbb{R}^n \rightarrow [0, \infty) $ is such that
            $ \forall \mathbf{x} \in \mathbb{R}^n $,
            \begin{equation*}
                f_X(\mathbf{x}) \triangleq \frac{1}{(2\pi)^{n / 2}}
                e^{-\frac{1}{2}\mathbf{x}^\top\mathbf{x}} =
                \prod_{k = 1}^n\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x_k^2} =
                \prod_{k = 1}^nf_{Z_k}(x_1)
            \end{equation*}
            Note that $ X $ has mean $ \mathbf{0} \in \mathbb{R}^n $ and
            covariance matrix $ \mathbf{I} \in \mathbb{R}^{n \times n} $.
        \end{itemize}
	\end{itemize}
\end{frame}

\subsection{Moments}

\begin{frame}{Moments}
    \begin{itemize}
        \item
        \textit{Definition.} Given a random variable
        $ X : \Omega \rightarrow \mathbb{F} $, its \textit{expectation} is
        denoted as $ \mathbb{E}[X] $ or $ \mu_X $. If $ X $ discrete with pmf
        $ p_X : \mathbb{F} \rightarrow [0, 1] $,
        \begin{equation*}
            \mathbb{E}[X] \triangleq \sum_{x \in \mathbb{F}}xp_X(x) =
            \sum_{x \in \mathbb{F}}x\mathbb{P}\{X = x\}
        \end{equation*}
        If $ X $ continuous with pdf
        $ f_X : \mathbb{F} \rightarrow [0, \infty) $, then
        \begin{equation*}
            \mathbb{E}[X] \triangleq \int_\mathbb{F}xf_X(x)\,dx
        \end{equation*}
        For $ \mathbb{E}[X] $ to exist, finite $ \mathbb{E}|X| $ is required.

        \item
        \textit{Theorem. Law of the unconscious statistician.} Let
        $ X : \Omega \rightarrow \mathbb{F} $ with
        density\footnotemark\footnotetext{
            Also true if $ X $ discrete with pmf $ p_X $.        
        } $ f_X $ and let
        $ Y \triangleq h(X) $, $ h : \mathbb{F} \rightarrow \mathbb{F}' $. Then,
        \begin{equation*}
            \mathbb{E}[Y] = \mathbb{E}[h(X)] = \int_\mathbb{F}h(x)f_X(x)\,dx
        \end{equation*}
    \end{itemize}

    % adjust spacing for footnote
    \medskip

\end{frame}

\begin{frame}{Moments}
    \begin{itemize}
        \item
        \textit{Definition.} Given a random variable $ X : \Omega \rightarrow
        \mathbb{R} $, the \textit{variance} of $ X $ is denoted as
        $ \operatorname{Var}(X) $, where $ \operatorname{Var}(X) \triangleq
        \mathbb{E}\left[(X - \mu_X)^2\right] $.

        \item
        \textit{Definition.} Given a random variable $ X : \Omega \rightarrow
        \mathbb{R} $, the \textit{standard deviation} of $ X $ is
        $ \sigma_X \triangleq \sqrt{\operatorname{Var}(X)} $.

        \item
        \textit{Definition.} Given random variables
        $ X : \Omega \rightarrow \mathbb{R} $,
        $ Y : \Omega \rightarrow \mathbb{R} $, their \textit{covariance} is
        $ \operatorname{cov}(X, Y) \triangleq
        \mathbb{E}[(X - \mu_X)(Y - \mu_Y)] $.

        \item
        \textit{Definition.} Given random variables $ X : \Omega \rightarrow
        \mathbb{R} $, $ Y : \Omega \rightarrow \mathbb{R} $, their
        \textit{Pearson correlation} is $ \rho_{X, Y} \in [-1, 1] $ such that
        \begin{equation*}
            \rho_{X, Y} \triangleq \mathbb{E}\left[
                \left(\frac{X - \mu_X}{\sigma_X}\right)
                \left(\frac{Y - \mu_Y}{\sigma_Y}\right)
            \right] = \frac{\operatorname{cov}(X, Y)}{\sigma_X\sigma_Y}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Moments}
    \begin{itemize}
        \item
        \textit{Definition.} Given a random variable $ X : \Omega \rightarrow
        \mathbb{R}^n $, the \textit{covariance matrix} of $ X $ is
        $ \mathbf{C}_X \triangleq \mathbb{E}\left[(X - \mu_X)
        (X - \mu_X)^\top\right] \in \mathbb{R}^{n \times n} $, where
        $ \mathbf{C}_X \succeq \mathbf{0} $ (positive semidefinite),
        $ \mathbf{C}_X = \mathbf{C}_X^\top $ (symmetric).
        \begin{equation*}
            \mathbf{C}_X = \begin{bmatrix}
                \ \sigma_{X_1}^2 & \ldots & \operatorname{cov}(X_1, X_n) \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ \operatorname{cov}(X_n, X_1) & \ldots & \sigma_{X_n}^2 \
            \end{bmatrix}
        \end{equation*}

        \item
        \textit{Definition.} Given a random variable $ X : \Omega \rightarrow
        \mathbb{R}^n $, the \textit{correlation matrix} of $ X $ is
        $ \mathbf{R}_X \triangleq \mathbf{\Sigma}_X^{-1}\mathbf{C}_X
        \mathbf{\Sigma}_X^{-1} \in \mathbb{R}^{m \times n} $, $ \mathbf{R}_X
        \succeq \mathbf{0} $, $ \mathbf{R}_X = \mathbf{R}_X^\top $.
        $ \mathbf{\Sigma}_X \in \mathbb{R}^{n \times n} $ is a diagonal matrix
        with $ \sigma_{X_1}, \ldots \sigma_{X_n} $ on the diagonal.
        \begin{equation*}
            \mathbf{R}_X = \begin{bmatrix}
                \ \sigma_{X_1}^{-1} & \ldots & 0 \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ 0 & \ldots & \sigma_{X_n}^{-1} \
            \end{bmatrix} \mathbf{C}_X \begin{bmatrix}
                \ \sigma_{X_1}^{-1} & \ldots & 0 \ \\
                \ \vdots & \ddots & \vdots \ \\
                \ 0 & \ldots & \sigma_{X_n}^{-1} \
            \end{bmatrix}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Moments}
    \begin{itemize}
        \item
        \textit{Definition.} Given a random variable $ X : \Omega \rightarrow
        \mathbb{R} $, if $ \mathbb{E}\big|X^k\big| < \infty $,
        $ k \in \mathbb{N} $, the $ k $\textit{th raw moment} of $ X $ exists
        \cite{wasserman_stats} and is $ \mathbb{E}\big[X^k\big] $.

        \item
        \textit{Definition.} Given a random variable $ X : \Omega \rightarrow
        \mathbb{R} $, the $ k $\textit{th central moment} of $ X $ is
        $ \mathbb{E}\left[(X - \mu_X)^k\right] $, $ k \in \mathbb{N} $.

         \item
         \textit{Properties.}
         \begin{itemize}
             \item
             \textit{Linearity of expectation.} For random variables
             $ X_1, \ldots X_n $, constants
             $ a_0, a_1, \ldots a_n \in \mathbb{R} $,
             $ \mathbb{E}\left[a_0 + \sum_{k = 1}^na_kX_k\right] =
             a_0 + \sum_{k = 1}^na_k\mathbb{E}[X_k] $.

            \item
            \textit{Expectation and independence.} For scalar and mutually
            independent random variables $ X_1, \ldots X_n $,
            $ \mathbb{E}\left[\prod_{k = 1}^nX_k\right] =
            \prod_{k = 1}^n\mathbb{E}[X_k] $.

            \item
            \textit{Variance of translated sum of random variables.} For scalar random
            variables $ X_1, \ldots X_n $, constants
            $ a_0, a_1, \ldots a_n \in \mathbb{R} $,
            \begin{equation*}
                \operatorname{Var}\left(a_0 + \sum_{k = 1}^na_kX_k\right) =
                \sum_{k = 1}^n\operatorname{Var}(X_k) +
                2\sum_{i = 1}^n\sum_{j = 1}^{i - 1}\operatorname{cov}(X_i, X_j)
            \end{equation*}
         \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Moments}
    \begin{itemize}
        \item
        \textit{Useful identities.} For $ X : \Omega \rightarrow \mathbb{R} $,
        $ Y : \Omega \rightarrow \mathbb{R} $, $ A \subseteq \Omega $,
        \begin{equation*}
            \begin{aligned}
                \operatorname{Var}(X) & = \mathbb{E}\big[X^2\big] - \mu_X^2 \\
                \operatorname{cov}(X, Y) & = \mathbb{E}[XY] - \mu_X\mu_Y \\
                \mathbb{E}[\mathbb{I}_A(X)] & = \mathbb{P}\{X \in A\}
            \end{aligned}
        \end{equation*}

        \item
        \textit{Useful identities (cont.)}. For $ X : \Omega \rightarrow
        \mathbb{R}^n $, $ \mathbf{a} \in \mathbb{R}^n $,
        \begin{equation*}
            \begin{aligned}
                \mathbf{C}_X & = \mathbb{E}\left[XX^\top\right] -
                    \mu_X\mu_X^\top \\
                \mathbb{E}\left[\mathbf{a}^\top X\right] & =
                    \mathbf{a}^\top\mu_X \\
                \operatorname{Var}\left(\mathbf{a}^\top X\right) & =
                    \mathbf{a}^\top\mathbf{C}_X\mathbf{a}
            \end{aligned}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{References}
    \bibliographystyle{acm}
    \bibliography{../master_bib}
\end{frame}

\end{document}
