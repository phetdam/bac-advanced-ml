%%% exercise 08 %%%
% template modified by Derek Huang, original by Sean Cox.
\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, enumitem, fancyhdr, tikz}
% get rid of paragraph indent
\setlength{\parindent}{0 pt}
% allow section.equation numbering
\numberwithin{equation}{section}
% allows you to copy-paste code segments. requires pygments to be installed
% natively, i.e. not just in a Python venv! for example, on WSL Ubuntu you
% need to sudo apt-get install python3-pygments (unfortunately, the version
% will be a bit dated) and cannot rely on your venv Pygments version.
\usepackage{minted}
% makes clickable links to sections
\usepackage{hyperref}
% make the link colors blue, as well as cite colors. urls are magenta
\hypersetup{
    colorlinks, linkcolor = blue, citecolor = blue, urlcolor = magenta
}
% fancy pagestyle so we can use fancyhdr for fancy headers/footers
\pagestyle{fancy}
% add logo in right of header. note that you will have to adjust logo path!
\fancyhead[R]{\includegraphics[scale = 0.15]{../bac_logo1.png}}
% don't show anything in the left and center header
\fancyhead[L, C]{}
% give enough space for logo by reducing top margin height, head separator,
% increasing headerheight. see Figure 1 in the fancyhdr documentation. if
% \topmargin + \headheight + \headsep = 0, original text margins unchanged.
\setlength{\topmargin}{-60 pt}
\setlength{\headheight}{50 pt}
\setlength{\headsep}{10 pt}
% remove decorative line in the fancy header
\renewcommand{\headrulewidth}{0 pt}

% title, author + thanks, date
\title{Exercise 8}
\author{Derek Huang\thanks{NYU Stern 2021, BAC Advanced Team.}}
\date{June 10, 2021}

% shortcut commands. the % strips additional spacing.
\newcommand{\pytest}{\href{https://docs.pytest.org/en/stable/}{pytest}}
\newcommand{\pdb}{%
    \href{https://docs.python.org/3/library/pdb.html}{\texttt{pdb}}%
}
\newcommand{\minimize}{%
    \href{%
        https://docs.scipy.org/doc/scipy/reference/generated/%
        scipy.optimize.minimize.html%
    }{\texttt{scipy.optimize.minimize}}%
}
\newcommand{\minimizetrustconstr}{%
    \href{%
        https://docs.scipy.org/doc/scipy/reference/optimize.%
        minimize-trustconstr.html%
    }{\texttt{minimize(method="trust-constr")}}%
}
\newcommand{\coomatrix}{%
    \href{%
        https://docs.scipy.org/doc/scipy/reference/generated/%
        scipy.sparse.coo_matrix.html%
    }{\texttt{scipy.sparse.coo\_matrix}}%
}
\newcommand{\Bounds}{%
    \href{%
        https://docs.scipy.org/doc/scipy/reference/generated/%
        scipy.optimize.Bounds.html%
    }{\texttt{Bounds}}%
}
\newcommand{\LinearConstraint}{%
    \href{%
        https://docs.scipy.org/doc/scipy/reference/generated/%
        scipy.optimize.LinearConstraint.html%
    }{\texttt{LinearConstraint}}%
}

\begin{document}

\maketitle
% need to include this after making title to undo the automatic
% \thispagestyle{plain} command that is issued.
\thispagestyle{fancy}

% remove some space between title and first section to keep text on one page.
% comment this line out or adjust the space reduction as needed. generally,
% try to remove as little space as possible if you are trying to keep content
% on one page, else comment this line out if content spans multiple pages.
%\vspace{-20 pt}

\section{Introduction}

The goal of this exercise is to implement a hinge loss linear support vector classifier, solving both the primal and dual formulations using \minimize{} with \texttt{method="trust-constr"}.

\section{Instructions}

\subsection{General}

The \texttt{exercise\_08.py} file contains the \texttt{LinearSVC} class, unit
tests, and \pytest{} fixtures. Complete the \texttt{fit} method of the 
\texttt{LinearSVC} class by replacing
\texttt{\#\#\# your code goes here \#\#\#} comment blocks with appropriate
Python code. Note that some comment blocks inside \texttt{if} statements may
contain the \texttt{pass} statement, which is included only to silence
language parser/linter complaints about improper block indentation.

\medskip

Your code \textbf{must} be written in the areas marked by these blocks. Do
\textbf{not} change any of the pre-written code. The exercise is complete
$ \Leftrightarrow $ \texttt{pytest /path/to/exercise\_08.py} executes with
zero test failures.

\subsection{Using \texttt{trust-constr}}

For this exercise, we solve both the primal and dual linear SVC problems
using \minimize{}, the \textit{de facto} standard Python open-source
minimization routine, using a trust region method\footnote{
    We did not cover trust region methods last lecture, but knowledge of how
    they work is not needed for this exercise.
} for constrained optimization, i.e. the \texttt{trust-constr} solver, with
analytical gradient and Hessian. To do so, there are a few things that must
be done when invoking \minimize.

\begin{enumerate}
    \item
    \texttt{"trust-constr"} must be passed to the \texttt{method} keyword
    argument of \minimize.

    \item
    A function that returns the gradient of the required objective
    \texttt{fun} must be passed to the \texttt{jac} keyword argument and a
    function that returns the Hessian of \texttt{fun} must be passed to the
    \texttt{hess} keyword argument.

    \item
    Bounds on the optimization variable must be specified. When using an
    instance of the \Bounds{} class, the recommended practice, as the
    \Bounds{} class provides an easier way to express bounds compared to a
    list of tuples, \texttt{keep\_feasible} should not be specified and left
    to take its default value \texttt{False}.

    \item
    Problem constraints must be specified. Written in matrix-vector form as
    a function of the optimization variable, both primal and dual constraints
    are linear and can thus be expressed with a single \LinearConstraint{}
    instance with \texttt{keep\_feasible=False}.

    \item
    Solver convergence options must be specified if necessary, passed as a
    \texttt{dict} to the \texttt{options} keyword argument. The documentation
    for \minimizetrustconstr{} enumerates the key-value pairs that can be
    specified in the \texttt{dict} passed to \texttt{options} when
    \texttt{method="trust-constr"}.
\end{enumerate}

Here we show an example of using \minimize{} with
\texttt{method="trust-constr"} to solve a mean-variance portfolio
optimization problem, i.e. finding the portfolio
$ \mathbf{w}^* \in \mathbb{R}^n $ that solves the convex QP
\begin{equation} \label{eq:max_norm_port_opt}
    \begin{array}{ll}
        \displaystyle\min_{\mathbf{w}} &
            \frac{1}{2}\mathbf{w}^\top\mathbf{Cw} \\
        \text{s.t.} & \mathbf{1}^\top\mathbf{w} = 1 \\
            & \Vert\mathbf{w}\Vert_\infty \le \tau
    \end{array}
\end{equation}

Here $ \mathbf{C} \succ \mathbf{0} \in \mathbb{R}^{n \times n} $ is the
covariance matrix of the asset returns and $ \tau > 0 $ limits the maximum
fraction of wealth that can be invested in any asset. Assume that
$ \mathbf{C} $, represented in code by \texttt{cov}, has already been
computed and has shape \texttt{(n\_assets, n\_assets)} and that $ \tau $,
represented in code with \texttt{thres}, has been set to some positive value.
Furthermore, suppose that we want \texttt{trust-constr} to run for only 800
iterations with \texttt{gtol=1e-7} and initial guess $ \frac{1}{n}\mathbf{1}
\in \mathbb{R}^n $. We can then appropriately solve
(\ref{eq:max_norm_port_opt}) numerically using

\begin{minted}[autogobble]{python3}
import numpy as np
from scipy.optimize import Bounds, LinearConstraint, minimize
# objective, gradient, hessian
fobj = lambda w: 0.5 * w @ cov @ w
fgrad = lambda w: cov @ w
fhess = lambda w: cov
# linear equality constraint, max norm constraint expressed using Bounds
eq_cons = LinearConstraint(np.ones(n_assets), 1, 1)
norm_bd = Bounds(-thres, thres)
# get optimization result and print computed portfolio
res = minimize(
    fobj, np.ones(n_assets) / n_assets, method="trust-constr", jac=fgrad,
    hess=fhess, bounds=norm_bd, constraints=eq_cons,
    options=dict(gtol=1e-7, maxiter=800)
)
print(res.x)
\end{minted}

However, since the linear SVC primal problem requires minimization over
multiple variables, we must reformulate the primal in order to solve using
\minimize.

\subsection{Primal reformulation} \label{sec:primal_reform}

Let $ \mathbf{X} \in \mathbb{R}^{N \times d} $ be the matrix of training
points, where $ \mathbf{x}_k^\top $ is the $ k $th row, and let
$ \mathbf{y} \in \{-1, 1\}^N $ be the response vector of labels. For
convenience, define the ``signed'' data matrix
$ \mathbf{Z} \in \mathbb{R}^{N \times d} $, where
\begin{equation} \label{eq:signed_X}
    \mathbf{Z} \triangleq \begin{bmatrix}
        \ y_1\mathbf{x}_1^\top \ \\ \ \vdots \ \\ \ y_N\mathbf{x}_N^\top \
    \end{bmatrix}
\end{equation}

We can then concisely write the primal linear SVC problem using
$ \mathbf{Z} $ defined in (\ref{eq:signed_X}) as
\begin{equation} \label{eq:primal_svc_opt}
    \begin{array}{ll}
        \displaystyle\min_{\mathbf{w}, b, \xi} &
            \frac{1}{2}\Vert\mathbf{w}\Vert_2^2 +
            C\mathbf{1}^\top\xi \\
        \text{s.t.} & \mathbf{Zw} + b\mathbf{y} \succeq
            \mathbf{1} - \xi  \\
            & \xi \succeq \mathbf{0}
    \end{array}
\end{equation}

Here we are jointly optimizing over $ \mathbf{w} \in \mathbb{R}^d $,
$ b \in \mathbb{R} $, $ \xi \in \mathbb{R}^N $, where $ \xi $ is the vector
of slack variables. However, we cannot directly send (\ref{eq:primal_svc_opt})
to \minimize, as the routine requires that the objective be a function of a
single variable, say $ \beta $, but we can define $ \beta \in
\mathbb{R}^{d + N + 1} $ such that $ \beta^\top \triangleq
\begin{bmatrix} \ \mathbf{w}^\top & b & \xi^\top \ \end{bmatrix} $. Letting
$ f : \mathbb{R}^{d + N + 1} \rightarrow \mathbb{R} $ be the objective, a
function of $ \beta $, we have that $ \forall \beta \in
\mathbb{R}^{d + N + 1} $,
\begin{equation} \label{eq:primal_obj_joint}
    f(\beta) \triangleq \frac{1}{2}\beta^\top\mathbf{D}\beta +
    \mathbf{a}^\top\beta
\end{equation}

Here the diagonal matrix $ \mathbf{D} \in \mathbb{R}^{(d + N + 1) \times
(d + N + 1)} $ and vector $ \mathbf{a} \in \mathbb{R}^{d + N + 1} $ are such
that
\begin{equation} \label{eq:primal_hess_and_lin_joint}
    \mathbf{D} \triangleq \begin{bmatrix}
        \ \mathbf{I}_d & \mathbf{0}_{d \times (N + 1)} \ \\
        \ \mathbf{0}_{(N + 1) \times d} & \mathbf{0}_{(N + 1) \times (N + 1)} \
    \end{bmatrix} \quad\quad \mathbf{a} \triangleq \begin{bmatrix}
        \ \mathbf{0}_{d + 1} \ \\ \ C\mathbf{1}_N \
    \end{bmatrix}
\end{equation}

From (\ref{eq:primal_obj_joint}), it's then clear that $ f $ is a
convex\footnote{
    Since $ f $ is quadratic, it is convex $ \Leftrightarrow \mathbf{D}
    \succeq \mathbf{0} $. We know that $ \mathbf{D} \succeq \mathbf{0} $ since
    $ \forall \beta \in \mathbb{R}^{d + N + 1} $, we have
	\begin{equation*}
	    \beta^\top\mathbf{D}\beta = \begin{bmatrix}
	        \ \mathbf{w}^\top & b & \xi^\top \    
	    \end{bmatrix}\begin{bmatrix}
	        \ \mathbf{w} \ \\ \ 0 \ \\ \ \mathbf{0}_N \
	    \end{bmatrix} = \Vert\mathbf{w}\Vert_2^2 \ge 0
	\end{equation*}
} quadratic function of $ \beta $, so $ \nabla f $ and $ \nabla^2 f $ are
such that
\begin{equation} \label{eq:primal_grad_hess_joint}
    \nabla f(\beta) \triangleq \mathbf{D}\beta + \mathbf{a} \quad\quad
    \nabla^2f(\beta) \triangleq \mathbf{D}
\end{equation}

Now, having reformulated the objective, gradient, and Hessian of
(\ref{eq:primal_svc_opt}) to be functions of $ \beta \in
\mathbb{R}^{d + N + 1} $, we can also write the linear constraint in
(\ref{eq:primal_svc_opt}) in terms of $ \beta $. Putting the optimization
variables on one side, the constraint in terms of $ \mathbf{w} $, $ b $,
$ \xi $ is $ \mathbf{Zw} + b\mathbf{y} + \xi \succeq \mathbf{1} $, which in
terms of $ \beta $ can be written as $ \mathbf{A}\beta \succeq \mathbf{1} $,
where the linear constraint coefficient matrix $ \mathbf{A} \in
\mathbb{R}^{N \times (d + N + 1)} $ is such that
\begin{equation} \label{eq:primal_cons_mat_joint}
    \mathbf{A} \triangleq \begin{bmatrix}
        \ \mathbf{Z} & \mathbf{y} & \mathbf{I}_N \
    \end{bmatrix}
\end{equation}

Note that only $ \xi $ is bounded, in particular (\ref{eq:primal_svc_opt})
requires $ \xi \succeq \mathbf{0} $, while $ \mathbf{w} $, $ b $ are unbounded.
Although there is no easy way to express partial bounding of $ \beta $
mathematically, it is not hard to use the \Bounds{} class to require only the
$ \xi $ block of $ \beta $ to be nonnegative while leaving the $ \mathbf{w} $,
$ b $ blocks unbounded.

\subsection{Solving the primal}

To solve the reformulation of (\ref{eq:primal_svc_opt}) described in
\nameref{sec:primal_reform} using \minimize, there are a few particulars that
must be paid attention to in order to ensure correctness and efficiency.

\medskip

\begin{enumerate}
    \item
    The initial guess $ \beta_0 \in \mathbb{R}^{d + N + 1} $ provided to the
    \texttt{x0} positional argument of \minimize{} should be $ \mathbf{0} $,
    i.e. a \texttt{numpy.ndarray} shape
    \texttt{(n\_features + n\_samples + 1,)} filled with zeroes.

    \item
    $ f $, $ \nabla f $ involve the inverse regularization parameter $ C $,
    which corresponds to \texttt{self.C}.

    \item
    The primal Hessian $ \mathbf{D} $ defined in
    (\ref{eq:primal_hess_and_lin_joint}) is very sparse, as only the first
    $ d $ diagonal elements, which are all ones, are nonzero. We can save
    memory by storing its nonzero elements using a sparse matrix type, for
    example \coomatrix, and have $ \nabla^2 f $ simply return a reference to
    $ \mathbf{D} $. The memory required by $ \mathbf{D} $ is $ O(d) $ if
    $ \mathbf{D} $ is stored as a \coomatrix\footnote{
        Although COO sparse matrices do not directly support arithmetic
        operations, \minimizetrustconstr{} introduces a wrapper for the
        Hessian that converts sparse matrices to CSR sparse matrix format,
        which does support arithmetic.
    }, but is $ O((d + N + 1)^2) $ if $ \mathbf{D} $ is stored as a
    \texttt{numpy.ndarray}. Note that both $ f $, $ \nabla f $ can be
    evaluated without $ \mathbf{D} $.

    \item
    The linear constraint coefficient matrix $ \mathbf{A} $ defined in
    (\ref{eq:primal_cons_mat_joint}) is also sparse, as only $ N(d + 2) $
    elements out of $ N(d + N + 1) $ are nonzero, so memory can be saved by
    storing $ \mathbf{A} $ as a \coomatrix. The linear constraint
    $ \mathbf{A}\beta \succeq \mathbf{1} $ can be easily expressed as a
    \LinearConstraint{} with $ \mathbf{A} $ passed to \texttt{A}.

    \item
    \texttt{self.tol} and \texttt{self.max\_iter} must be respectively paired
    with the \texttt{gtol} and \texttt{maxiter} keys of the \texttt{dict} sent
    to the \texttt{options} keyword argument of \minimize.
\end{enumerate}

%\subsection{Solving the dual}

%The linear SVC dual problem can be written as
%\begin{equation*}
%    \begin{array}{ll}
%        \displaystyle\min_\alpha & \frac{1}{2}\alpha^\top\mathbf{ZZ}^\top\alpha
%            - \mathbf{1}^\top\alpha \\
%        \text{s.t.} & \alpha^\top\mathbf{y} = 0 \\
%            & \mathbf{0} \preceq \alpha \preceq C\mathbf{1}
%    \end{array}
%\end{equation*}

\subsection{Tips}

\begin{enumerate}
    % need to add more important tips later

    \item
    \href{%
        https://numpy.org/doc/stable/reference/generated/numpy.hstack.html%
    }{\texttt{numpy.hstack}} is very useful for creating matrices from smaller
    blocks. For example, let $ \mathbf{A} \in \mathbb{R}^{m \times n_1} $ be
    represented by \texttt{A}, a \texttt{numpy.ndarray} with shape
    \texttt{(m, n1)}, and let $ \mathbf{B} \in \mathbb{R}^{m \times n_2} $ be
    represented by \texttt{B}, a \texttt{numpy.ndarray} with shape
    \texttt{(m, n2)}. Then, \texttt{numpy.hstack((A, B))} returns a
    \texttt{numpy.ndarray} shape \texttt{(m, n1 + n2)} representing the block
    matrix $ \begin{bmatrix} \ \mathbf{A} & \mathbf{B} \ \end{bmatrix} $.

    \item
    Use NumPy functions whenever possible for efficiency, brevity, and ease
    of debugging.

    \item
    Invoke \pytest{} with the \texttt{--pdb} flag to start \pdb, the Python
    debugger. Doing so allows you to inspect the variables in the current call
    frame and look more closely at what went wrong.
\end{enumerate}

\end{document}